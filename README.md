## 1. Justificaci√≥n y descripci√≥n del proyecto.
### **Desarrolladores**

- Alejandro Fern√°ndez Barrionuevo
- Adri√°n Perogil Fern√°ndez
- Carlos L√≥pez Mu√±oz

### **T√≠tulo**

![image](https://github.com/user-attachments/assets/bbe87bcf-e2fa-4f2c-9f76-ba2a265287d9)


### **Descripci√≥n**

Un proyecto basado en detecci√≥n de obst√°culos donde, gracias al uso de YOLO, podemos detectar con una c√°mara todo tipo de objetos en la vida real.

Su uso escalable y la intenci√≥n con la que se hizo este proyecto fue para ayudar a personas con discapacidades visuales que mediante audio, guiara a las personas
y pudiera recibir un feedback en todo momento, as√≠ pudiendo caminar con mayor comodidad y seguridad. 

Como el tiempo que tuvimos era limitado y era muy ambicioso, nos vimos en la obligaci√≥n de pensar "niveles" para empezar con algo b√°sico hasta llegar al target.

### **C√≥digo fuente**

[WEB](https://github.com/imchopi/InnerVisionAI/tree/feature_alex)

[API](https://github.com/imchopi/API_InnerVisionAI)

### **Presentaci√≥n en formato PDF**

[InnerVisionAI.pdf](https://github.com/user-attachments/files/19059039/InnerVisionAI.pdf)

[CANVAS](https://www.canva.com/design/DAGghJUTLyQ/ZZKpm5k1CVVnxM2wO5-TVQ/edit?utm_content=DAGghJUTLyQ&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)

### **Enlace a la aplicaci√≥n web**

[P√°gina Web](https://innervisionai.netlify.app/home)

### **Recursos utilizados**

- Jira
- YOLOv5
- Roboflow
- Google Colab
- Flask
- React e Ionic
- TypeScript
- Socket.IO
- OpenAI API
- Node.js
- Axios
- GitHub
- Netlify
- Python
- HTML/CSS
- Markdown
- Base64
- CSV
- Git
- DeepSeek
- ChatGPT
- Diversos Foros

### **V√≠deo**

[VIDEO](https://drive.google.com/file/d/1wJosbON_d42RYheS3YLzwbOHQrZMqdRH/view?usp=sharing)


### **Porcentaje que le corresponde a cada miembro del trabajo realizado de dicho proyecto.**

- Alejandro Fern√°ndez Barrionuevo (33.33%)
- Adri√°n Perogil Fern√°ndez (33.33%)
- Carlos L√≥pez Mu√±oz (33.33%)

## 2. Obtenci√≥n de datos. Se debe especificar la fuente de los datos. Se indicar√° por qu√© medios se han obtenido (encuestas, sensores, scrapping, etc.). Los datos se deben cargar en una estructura que permita su posterior manipulaci√≥n y uso.

Para conseguir los datos que necesit√°bamos, decidimos extraer im√°genes de internet . Como nuestro modelo va a funcionar en tiempo real, quer√≠amos que reconociera objetos comunes en la empresa, como sillas, armarios y mesas ü™ëüì¶.

Por eso elegimos IKEA como fuente de informaci√≥n, ya que es una de las mejores referencias y sabemos que tiene justo lo que est√°bamos buscando .

Lo primero que hicimos fue instalar las librer√≠as necesarias üõ†Ô∏è. Al principio, como la web parec√≠a sencilla, pensamos que con BeautifulSoup ser√≠a suficiente, pero no fue as√≠.

Si quer√≠amos cargar todas las im√°genes, nos dimos cuenta de que hab√≠a un bot√≥n de "Mostrar m√°s", que era el encargado de cargar las im√°genes.

Para solucionar este problema, utilizamos Selenium, que nos permite interactuar con la web como si fu√©ramos un usuario real.

A continuaci√≥n, explicar√© las librer√≠as que usaremos para llevar a cabo la extracci√≥n de datos üîç.

### üìå Librer√≠as utilizadas para la extracci√≥n de datos

<img src="Imagenes/IKEA/Imagen-1.png" width="600" height="300">

Para poder extraer informaci√≥n de la web, utilizamos varias librer√≠as üìö que nos ayudar√°n en diferentes tareas:

os ‚Üí Nos permite interactuar con los archivos y carpetas de nuestro sistema.

time ‚Üí Se usa para pausar el c√≥digo y esperar antes de realizar ciertas acciones.

requests ‚Üí Nos ayuda a hacer peticiones a sitios web y obtener su contenido.

re ‚Üí Se usa para trabajar con patrones de texto, como encontrar enlaces o filtrar datos.
BeautifulSoup (de bs4) ‚Üí Nos permite extraer informaci√≥n de una p√°gina web de forma m√°s sencilla.

üöÄ Automatizaci√≥n con Selenium

webdriver ‚Üí Nos permite controlar un navegador web (como Chrome).

Service  ‚Üí Maneja la configuraci√≥n del navegador para Selenium.

Options ‚Üí Nos ayuda a establecer preferencias para el navegador (como abrirlo en modo 
invisible).

By ‚Üí Se usa para encontrar elementos dentro de la p√°gina web.

WebDriverWait ‚Üí Permite que el c√≥digo espere hasta que un elemento de la web aparezca.

expected_conditions (EC) ‚Üí Nos ayuda a esperar hasta que ciertas condiciones se cumplan, como que un bot√≥n sea visible antes de hacer clic.

## üîç Estructura del C√≥digo  

Lo siguiente que mostrar√© es un poco c√≥mo he **organizado** este c√≥digo üìú.  

Voy a explicar **cada parte** para que sea f√°cil de entender y seguir üìå.  

---

<img src="Imagenes/IKEA/Imagen-2.png" width="1200" height="400">

## Clase IKEA Scraper

Esta clase se encarga de **extraer informaci√≥n** de la web de **IKEA** üè†, espec√≠ficamente im√°genes de **armarios, mesas y sillas**.  

## üìå 1. Inicializaci√≥n (`__init__` method)  
Cuando creamos un objeto de esta clase, se ejecuta este c√≥digo autom√°ticamente.  

###  `self.category_urls`  
- Es un diccionario que contiene las **categor√≠as de muebles** y los enlaces de IKEA donde est√°n los productos üì¶.  
- Por ejemplo, para obtener **armarios**, el c√≥digo buscar√° en:  
  - `"https://www.ikea.com/es/es/cat/armarios-19053/"`  

###  `self.headers`  
- Sirve para **simular** que el navegador es un usuario real y evitar bloqueos de la web.  
- Aqu√≠ se usa un **"User-Agent"** (que le dice a la web que estamos usando un navegador como Chrome o Firefox).  

---

## 2. Configuraci√≥n del navegador con Selenium  
Como la web tiene **botones interactivos**, usamos **Selenium** para controlarla autom√°ticamente.  

### `chrome_options = Options()`  
- Nos permite **configurar** c√≥mo se abrir√° el navegador.  
- `chrome_options.add_argument("--start-maximized")` üñ•Ô∏è  
  - Abre el navegador en **pantalla completa** para evitar problemas con elementos ocultos.  

### `self.driver = webdriver.Chrome(...)`  
- Aqu√≠ estamos **iniciando Chrome** para que Selenium pueda interactuar con la web **como si fuera un usuario real**.  

---

<img src="Imagenes/IKEA/Imagen-3.png" width="800" height="300">

## Funci√≥n `create_directories`

Esta funci√≥n se encarga de **crear carpetas** en el sistema para almacenar las im√°genes de cada categor√≠a üóÇÔ∏è.

### ¬øC√≥mo funciona?

1. **Obtiene las categor√≠as**  
   - Usa `self.category_urls.keys()` para obtener la lista de categor√≠as disponibles (ejemplo: "sillas", "mesas", "armarios").  

2. **Crea una carpeta para cada categor√≠a**  
   - Recorre la lista de categor√≠as y construye la ruta donde se guardar√°n los datos.  
   - Usa `os.path.join(base_path, category)` para combinar la ruta base con el nombre de la categor√≠a.  

3. **Verifica si la carpeta existe** 
   - Si la carpeta **no existe**, se crea con `os.makedirs(path)`.  

4. **Devuelve la lista de categor√≠as**  
   - Retorna la lista de categor√≠as que se crearon.  

--- 

<img src="Imagenes/IKEA/Imagen-4.png" width="600" height="300"> 

## Funci√≥n `download_image`

Esta funci√≥n **descarga una imagen desde una URL y la guarda en el sistema** üì•üìÇ.

### üõ†Ô∏è ¬øC√≥mo funciona?

1. **Hace una solicitud a la URL** 
   - Utiliza `requests.get(url, headers=self.headers)` para obtener la imagen desde el enlace.  
   - Usa **`self.headers`** para simular una petici√≥n desde un navegador y evitar bloqueos.  

2. **Verifica que la imagen se haya descargado correctamente**   
   - Si el c√≥digo de respuesta (`response.status_code`) es `200`, significa que la imagen se descarg√≥ sin problemas.  

3. **Guarda la imagen en el archivo especificado** 
   - Abre el archivo en modo escritura binaria (`'wb'`).  
   - Escribe el contenido de la imagen en el archivo.  
   - Imprime un mensaje confirmando que la imagen se guard√≥ correctamente.  

4. **Manejo de errores** ‚ö†Ô∏è  
   - Si ocurre un error en la descarga, se captura con `except Exception as e`.  
   - Se imprime un mensaje indicando el problema y la funci√≥n devuelve `False`.  

---

<img src="Imagenes/IKEA/Imagen-5.png" width="1200" height="400"> 

## Funci√≥n `load_all_products`

Esta funci√≥n **carga todos los productos de la p√°gina web** haciendo clic en el bot√≥n `"Mostrar m√°s"` hasta que ya no haya m√°s productos disponibles .

### üõ†Ô∏è ¬øC√≥mo funciona?

1. **Abre la p√°gina web** 
   - Utiliza `self.driver.get(url)` para acceder a la URL proporcionada.  

2. **Espera a que la p√°gina cargue**  
   - Crea una espera con `WebDriverWait` para asegurarse de que los elementos est√©n listos antes de interactuar con ellos.  

3. **Bucle para cargar m√°s productos**   
   - Se usa un `while True` para hacer clic en `"Mostrar m√°s"` varias veces.  
   - **Busca el bot√≥n** con `wait.until(...)` y verifica que sea **clickeable**.  
   - **Ejecuta un clic** en el bot√≥n con `execute_script("arguments[0].click();", load_more_button)`.  
   - Muestra el mensaje `"Cargando m√°s productos..."` en la terminal.  
   - Espera `2 segundos` (`time.sleep(2)`) antes de intentar hacer clic nuevamente.  

4. **Manejo de errores** 
   - Si el bot√≥n `"Mostrar m√°s"` ya no est√° disponible, significa que **no hay m√°s productos**.  
   - En ese caso, se muestra el mensaje `"No hay m√°s productos para cargar."` y el bucle se detiene con `break`.

---

<img src="Imagenes/IKEA/Imagen-6.png" width="800" height="500"> 

## Funci√≥n `scrape_category`

Esta funci√≥n **extrae im√°genes de productos** de una **categor√≠a espec√≠fica** en IKEA y las guarda en una carpeta .

### üõ†Ô∏è ¬øC√≥mo funciona?

1. **Carga la p√°gina de la categor√≠a** 
   - Obtiene la URL de la categor√≠a desde `self.category_urls[category]`.  
   - Llama a `self.load_all_products(url)`, que se encarga de hacer clic en `"Mostrar m√°s"` hasta que todos los productos est√©n cargados.  

2. **Extrae los productos con `BeautifulSoup`**  
   - Obtiene el c√≥digo HTML de la p√°gina con `self.driver.page_source`.  
   - Busca todos los productos en la web usando `soup.find_all('div', class_='plp-fragment-wrapper')`.  

3. **Recorre cada producto y extrae su informaci√≥n**  
   - Para cada producto, intenta encontrar:  
     - La **imagen** (`<img>`) y su URL.  
     - El **nombre del producto** dentro de un `<span>`.  

4. **Limpia el nombre del producto** 
   - Si el nombre contiene caracteres especiales, los reemplaza con `_` usando `re.sub()`.  
   - Si no se encuentra un nombre, usa un nombre gen√©rico basado en la categor√≠a y el √≠ndice (`category_idx`).  

5. **Descarga la imagen**
   - Guarda la imagen en la ruta `save_path`, nombr√°ndola con el nombre limpio.  
   - Llama a `self.download_image(img_url, image_path)`, que descarga y guarda la imagen.  
   - Muestra un mensaje en la terminal confirmando la descarga.  

6. **Manejo de errores**   
   - Si hay un problema al procesar un producto, muestra un mensaje de error y pasa al siguiente.  

---

<img src="Imagenes/IKEA/Imagen-7.png" width="800" height="500"> 

## Funci√≥n `run`

Esta funci√≥n **ejecuta el scraping completo** en todas las categor√≠as definidas.

### üõ†Ô∏è ¬øC√≥mo funciona?

1. **Muestra un mensaje de inicio**  
   - Se imprime `"Iniciando proceso de scraping..."` para indicar que el proceso ha comenzado.  

2. **Crea los directorios para cada categor√≠a**   
   - Llama a `self.create_directories(base_path)`, que se encarga de crear carpetas donde se guardar√°n las im√°genes.  
   - Muestra en la terminal las categor√≠as para las cuales se han creado carpetas.  

3. **Realiza el scraping de cada categor√≠a**   
   - **Recorre todas las categor√≠as disponibles** usando un `for`.  
   - **Imprime un mensaje indicando que est√° iniciando el scraping** de esa categor√≠a.  
   - Genera la ruta donde se guardar√°n las im√°genes (`category_path`).  
   - Llama a `self.scrape_category(category, category_path)`, que extrae las im√°genes de la web.  
   - Muestra un mensaje indicando que el scraping de esa categor√≠a ha finalizado.  
   - **Espera 3 segundos (`time.sleep(3)`)** antes de pasar a la siguiente categor√≠a.  

4. **Cierra el navegador** 
   - Llama a `self.driver.quit()`, lo que finaliza el uso de Selenium y cierra el navegador.  

--- 

<img src="Imagenes/IKEA/Imagen-8.png" width="600" height="300"> 

## Ejecuci√≥n del scraper en local

Este bloque de c√≥digo **inicia el proceso de scraping** cuando ejecutamos el script en nuestro ordenador.

### üõ†Ô∏è ¬øC√≥mo funciona?

1. **Verifica si el script se est√° ejecutando directamente** 
   - La condici√≥n `if __name__ == "__main__":` se asegura de que el c√≥digo **solo se ejecute** cuando ejecutamos el archivo directamente y no si se importa como m√≥dulo en otro script.  

2. **Crea una instancia del scraper** 
   - Se inicializa un objeto `IKEAScraper`, indicando la ruta del **ChromeDriver** (`chromedriver.exe`).  
   - `ChromeDriver` es necesario para que **Selenium** pueda controlar el navegador.  

3. **Ejecuta el proceso completo de scraping**  
   - Se llama a `scraper.run()`, que se encargar√° de:
     - Abrir el navegador.
     - Extraer los productos de todas las categor√≠as.
     - Descargar las im√°genes.
     - Guardarlas en carpetas organizadas.

---

## Ampliando el scraping: objetos m√°s peque√±os  

Despu√©s de extraer informaci√≥n sobre **muebles**, decidimos tambi√©n obtener datos de **objetos m√°s peque√±os** que se pueden encontrar dentro de la empresa, como **teclados, port√°tiles y ratones** ‚å®Ô∏èüíªüñ±Ô∏è.  

### Cambio de fuente: PCComponentes  
Para estos productos, optamos por **extraer la informaci√≥n de PCComponentes**.  

Sin embargo, pronto nos dimos cuenta de una **diferencia clave** con la web de IKEA:  
üîπ **No ten√≠a un bot√≥n de "Cargar m√°s"** como IKEA.  
üîπ En su lugar, hab√≠a un **bot√≥n para pasar a la siguiente p√°gina**.  

### üöß Problema encontrado: Bloqueo de bots  
Cuando intentamos **pasar a la siguiente p√°gina**, la web nos detectaba como **un bot** üö´ü§ñ, impidiendo que sigui√©ramos navegando.  

### Soluci√≥n ingeniosa  
Para **evitar el bloqueo**, encontramos una soluci√≥n **sencilla pero efectiva**:  

En lugar de hacer clic en **"Siguiente p√°gina"**, usamos un **bucle** que:  
1. **Scrapea** la informaci√≥n de la p√°gina actual.  
2. **Cierra el navegador** completamente.  
3. **Vuelve a abrirlo** en la siguiente p√°gina con la nueva URL.  

üìå **Resultado:** La web no detectaba que √©ramos un bot, permiti√©ndonos extraer la informaci√≥n sin problemas üéØ.  

---

## C√≥digo para extraer datos de PCComponentes  

En esta situaci√≥n, utilizaremos **tres c√≥digos similares** , pero con algunos **detalles espec√≠ficos** cambiados para que cada uno **interact√∫e con su categor√≠a correspondiente**.  

üìå **Las tres categor√≠as a scrapear son:**  
- **Teclados** 
- **Port√°tiles**  
- **Ratones**   

Dado que los c√≥digos son casi **id√©nticos**, solo mostrar√© el c√≥digo correspondiente a **los ratones**, ya que la l√≥gica es la misma para los otros dos casos, con peque√±os ajustes.  

---

üìå **A continuaci√≥n, el c√≥digo para extraer informaci√≥n de los ratones en PCComponentes**.

<img src="Imagenes/PCCOMPONENTES/Ratones/imagen-1.png" width="600" height="300"> 

##  Librer√≠as utilizadas para el scraping de PCComponentes  

Para poder extraer los datos de **PCComponentes**, utilizamos varias **librer√≠as** que nos ayudar√°n en diferentes tareas:  

### üîπ **Librer√≠as b√°sicas**
- **`requests`** ‚Üí Se usa para hacer peticiones a la web y obtener su contenido.  
- **`os`** ‚Üí Permite interactuar con los archivos y carpetas del sistema.  
- **`time`** ‚Üí Se usa para hacer pausas en el c√≥digo y evitar bloqueos por sobrecarga de peticiones.  

### **BeautifulSoup: An√°lisis del HTML**
- **`BeautifulSoup (bs4)`** ‚Üí Se encarga de **extraer informaci√≥n del c√≥digo HTML** de la web.  

### **Selenium: Automatizaci√≥n del navegador**  
Dado que la web de **PCComponentes** requiere **interacci√≥n con la paginaci√≥n**, usamos **Selenium** para controlarla:  

- **`webdriver`** ‚Üí Controla el navegador (Chrome en este caso).  
- **`Service`** ‚Üí Configura la ejecuci√≥n del navegador de forma autom√°tica.  
- **`Options`** ‚Üí Permite configurar c√≥mo se abre el navegador (ejemplo: en modo sin interfaz).  
- **`By`** ‚Üí Se usa para buscar elementos dentro de la web (ejemplo: botones o im√°genes).  
- **`WebDriverWait`** ‚Üí Permite que el c√≥digo **espere** hasta que un elemento de la web se cargue correctamente.  
- **`expected_conditions (EC)`** ‚Üí Se usa para definir condiciones antes de interactuar con elementos de la web (como esperar a que un bot√≥n sea clickeable).  

---

<img src="Imagenes/PCCOMPONENTES/Ratones/imagen-2.png" width="1600" height="400">  

## Clase `PCscrapper`

Esta clase se encarga de **extraer informaci√≥n de los ratones en PCComponentes**.  

### üõ†Ô∏è **Inicializaci√≥n de la clase (`__init__` method)**  

Cuando creamos un objeto de esta clase, se ejecuta el c√≥digo de inicializaci√≥n que:  

### **Define las URLs de las p√°ginas a scrapear**  
- **`self.category_urls`** almacena un **diccionario de URLs**, donde cada clave es el nombre de la p√°gina y el valor es la URL correspondiente.  
- Se usa un **bucle con `range(1, 29)`**, lo que indica que se van a recorrer **28 p√°ginas** (de la 1 a la 28).  
- La URL cambia din√°micamente con `?page={page}`, lo que permite navegar por las diferentes p√°ginas sin hacer clic en "Siguiente".  

### **Define las cabeceras (`headers`)**  
- **`self.headers`** almacena un **"User-Agent"**, que hace que la petici√≥n parezca de un navegador real.  
- Esto ayuda a **evitar bloqueos** por parte de la web al detectar actividad sospechosa de bots ü§ñüö´.  

---

<img src="Imagenes/PCCOMPONENTES/Ratones/imagen-3.png" width="600" height="300"> 

## Funci√≥n `create_directories`

Esta funci√≥n se encarga de **crear carpetas** en el sistema para almacenar las im√°genes de cada categor√≠a .

### üõ†Ô∏è ¬øC√≥mo funciona?

1. **Obtiene las categor√≠as** 
   - Usa `self.category_urls.keys()` para obtener la lista de categor√≠as disponibles (ejemplo: `"ratones1"`, `"ratones2"`, `"ratones3"`, etc.).  

2. **Crea una carpeta para cada categor√≠a**  
   - Recorre la lista de categor√≠as y **construye la ruta** donde se guardar√°n los datos.  
   - Usa `os.path.join(base_path, category)` para combinar la ruta base con el nombre de la categor√≠a.  

3. **Verifica si la carpeta existe** 
   - Si la carpeta **no existe**, se crea con `os.makedirs(path)`.  

4. **Devuelve la lista de categor√≠as** 
   - Retorna la lista de categor√≠as que se crearon.  

---

<img src="Imagenes/PCCOMPONENTES/Ratones/imagen-4.png" width="800" height="300"> 

## Funci√≥n `download_image`

Esta funci√≥n **descarga una imagen desde una URL y la guarda en el sistema**.

### üõ†Ô∏è **¬øC√≥mo funciona?**

1. **Hace una solicitud a la URL** 
   - Usa `requests.get(url, headers=self.headers)` para obtener la imagen desde el enlace.  
   - Incluye **cabeceras HTTP (`headers`)** para **simular una petici√≥n real de un navegador** y evitar bloqueos.  

2. **Verifica que la imagen se haya descargado correctamente**  
   - Si el c√≥digo de respuesta (`response.status_code`) es **200**, significa que la imagen se descarg√≥ sin problemas.  

3. **Crea el directorio si no existe** 
   - Usa `os.makedirs(os.path.dirname(path), exist_ok=True)` para asegurarse de que la carpeta donde se guardar√° la imagen **exista**.  
   - `exist_ok=True` evita errores si la carpeta ya est√° creada.  

4. **Guarda la imagen en el archivo especificado**  
   - Abre el archivo en modo escritura binaria (`'wb'`).  
   - Escribe el contenido de la imagen en el archivo.  
   - Imprime un mensaje confirmando que la imagen se guard√≥ correctamente.  

5. **Manejo de errores** 
   - Si ocurre un error en la descarga, se captura con `except Exception as e`.  
   - Se imprime un mensaje indicando el problema y la funci√≥n devuelve `False`.  

---

<img src="Imagenes/PCCOMPONENTES/Ratones/imagen-5.png" width="900" height="600"> 

## Funci√≥n `download_products_images`

Esta funci√≥n **descarga im√°genes de una p√°gina web de manera autom√°tica**.

### üõ†Ô∏è **¬øC√≥mo funciona?**

1. **Abre la p√°gina web en Chrome** 
   - Usa **Selenium** para abrir la URL en un navegador.  
   - Configura el navegador en **modo maximizado** para evitar problemas de visualizaci√≥n.  

2. **Acepta las cookies si es necesario**  
   - Busca el bot√≥n de "Aceptar cookies" y hace clic en √©l.  
   - Si el bot√≥n no est√° presente, **el c√≥digo sigue sin problemas**.  

3. **Espera a que las im√°genes se carguen**  
   - Usa `WebDriverWait` para asegurarse de que las im√°genes est√©n visibles antes de continuar.  
   - Utiliza **BeautifulSoup** para analizar el c√≥digo HTML de la p√°gina y extraer las im√°genes.  

4. **Busca y filtra im√°genes relevantes**   
   - Extrae todas las etiquetas `<img>` que tengan atributos `src` (enlace de imagen) y `alt` (descripci√≥n).  
   - Obtiene la URL de cada imagen.  
   - Si la URL es **relativa** (empieza con `/`), se convierte en una URL **completa** agregando el dominio.  

5. **Descarga y guarda las im√°genes**  
   - Genera un **nombre de archivo √∫nico** (`image_{i}.jpg`) y lo guarda en una carpeta seg√∫n su categor√≠a.  
   - Llama a la funci√≥n `self.download_image()` para almacenar la imagen correctamente.  

6. **Manejo de errores y cierre del navegador**   
   - Si hay un error durante la ejecuci√≥n, se muestra un mensaje con la URL afectada.  
   - Al final, **siempre se cierra el navegador** para liberar recursos.  

---

<img src="Imagenes/PCCOMPONENTES/Ratones/imagen-6.png" width="600" height="300">   

## Funci√≥n `run`

Esta funci√≥n **ejecuta el scraper** y se encarga de **organizar y descargar las im√°genes** de diferentes categor√≠as.

### üõ†Ô∏è **¬øC√≥mo funciona?**

1. **Define la carpeta base donde se guardar√°n las im√°genes**   
   - `base_path = 'images'` establece el directorio principal donde se almacenar√°n las im√°genes descargadas.  

2. **Crea las carpetas necesarias** 
   - Usa `self.create_directories(base_path)` para asegurarse de que la carpeta base **exista antes de guardar las im√°genes**.  
   - Si la carpeta ya existe, **evita errores y contin√∫a con la ejecuci√≥n**.  

3. **Recorre todas las categor√≠as de productos**   
   - Usa un **bucle `for`** para iterar sobre `self.category_urls.items()`, donde cada categor√≠a tiene una URL espec√≠fica.  
   - Extrae la `category` (nombre de la categor√≠a) y `url` (direcci√≥n web donde se encuentran las im√°genes).  

4. **Llama a la funci√≥n de descarga de im√°genes**  
   - `self.download_products_images(url, category)` procesa la p√°gina y **descarga las im√°genes correspondientes a cada categor√≠a**.  

---

<img src="Imagenes/PCCOMPONENTES/Ratones/imagen-7.png" width="600" height="300">  

## Bloque `if __name__ == "__main__"`

Este bloque de c√≥digo **ejecuta el programa en local** y **pone en marcha el scraper**.

### üõ†Ô∏è **¬øC√≥mo funciona?**

1. **Verifica que el script se est√° ejecutando directamente** 
   - La l√≠nea `if __name__ == '__main__':` **asegura que el c√≥digo solo se ejecute si este archivo es ejecutado directamente**, y **no si es importado como m√≥dulo en otro script**.  

2. **Crea una instancia del scraper** 
   - `scrapper = PCscrapper()` inicializa un objeto de la clase `PCscrapper`, que contiene toda la l√≥gica del scraping.  
   - Aqu√≠ se configuran las opciones y los par√°metros del scraper.  

3. **Ejecuta el scraper** 
   - `scrapper.run()` llama a la funci√≥n `run()` para iniciar la **descarga y organizaci√≥n de im√°genes** desde las p√°ginas web.  
   - Esto hace que **se ejecuten todos los procesos del scraper autom√°ticamente**.  

---

# üñ•Ô∏è **Scraping de datos de ratones en PCComponentes para Power BI** üìä  

Para la **visualizaci√≥n de datos en Power BI**, pens√© que ya ten√≠a las im√°genes, as√≠ que‚Ä¶ **¬øqu√© tan dif√≠cil podr√≠a ser obtener los datos de los ratones ?** 

Bueno, result√≥ ser **el mayor reto al que me he enfrentado**.  

### **El desaf√≠o**  
Por m√°s que intentaba localizar los datos, **cambiaba el nombre de las class una y otra vez**, pero nada funcionaba. Cuando estaba **a punto de rendirme**, tom√© aire, suspir√© y decid√≠ empezar **desde cero, paso a paso**.  

### **Paso a paso, desentra√±ando los datos**  
1Ô∏è‚É£ **Extraer el nombre**: Fue lo m√°s f√°cil, as√≠ que comenc√© por ah√≠.   
2Ô∏è‚É£ **Obtener el precio**: Perfecto, ahora ten√≠a **2 de los 5 datos** que necesitaba.   
3Ô∏è‚É£ **Conseguir la URL**: Tras algunos intentos, me di cuenta de que todas las URLs segu√≠an el mismo patr√≥n:  
   **Dominio de la web + Nombre del producto** üåêüîó  
   ¬°Un descubrimiento clave! De repente, **todas las URLs estaban listas**.  
4Ô∏è‚É£ **Valoraciones y cantidad de opiniones**: Ahora que entend√≠a la estructura, estos datos vinieron **de inmediato**.  

### üîÑ **El problema de la paginaci√≥n**  
Cuando intent√© pasar a la **siguiente p√°gina**, **el navegador no respond√≠a correctamente**.  
As√≠ que decid√≠ **cerrarlo y abrirlo en cada nueva p√°gina**. ¬°Funcion√≥! Ahora ten√≠a **27 archivos CSV**, uno por cada p√°gina de productos. 

### üèÜ **Lo que aprend√≠**  
‚úÖ A veces, lo mejor es **empezar desde cero y avanzar poco a poco**.  
‚úÖ **Observar patrones** en los datos puede hacerte la vida m√°s f√°cil.  
‚úÖ **Cada problema tiene una soluci√≥n** (aunque implique reiniciar el navegador üîÑ).  

Pr√≥ximo paso: **unir los 27 CSV en un solo archivo** y prepararlo para **Power BI**.  

---

## C√≥digo para la obtenci√≥n de datos. 

<img src="Imagenes/PCCOMPONENTES/Ratones-datos/imagen-1.png" width="600" height="300">  

## üì¶ **M√≥dulos importados en el scraper**  

Este bloque de c√≥digo **importa todas las librer√≠as necesarias** para el funcionamiento del scraper.   

### üõ†Ô∏è **¬øPara qu√© sirve cada una?**  

#### **Manejo del tiempo**  
- `import time` ‚Üí Permite **pausar la ejecuci√≥n** del c√≥digo en momentos clave.  

#### **Manejo de archivos CSV y texto**  
- `import csv` ‚Üí Para **guardar los datos extra√≠dos** en archivos CSV.  
- `import re` ‚Üí **Expresiones regulares**, √∫til para limpiar y estructurar texto.  
- `import unidecode` ‚Üí **Elimina acentos y caracteres especiales**, √∫til para URLs.  

#### **Manejo del sistema de archivos**  
- `import os` ‚Üí Permite **crear directorios y manejar rutas de archivos**.  

#### **Automatizaci√≥n del navegador con Selenium**  
- `from selenium import webdriver` ‚Üí Para **controlar el navegador web**.  
- `from selenium.webdriver.chrome.service import Service` ‚Üí Gestiona **el servicio de ChromeDriver**.  
- `from selenium.webdriver.chrome.options import Options` ‚Üí Configura **opciones avanzadas del navegador**.  
- `from selenium.webdriver.common.by import By` ‚Üí Facilita **la b√∫squeda de elementos en la web**.  
- `from selenium.webdriver.support.ui import WebDriverWait` ‚Üí Permite **esperar a que aparezcan elementos en la p√°gina**.  
- `from selenium.webdriver.support import expected_conditions as EC` ‚Üí Define **condiciones espec√≠ficas para esperar elementos** (ejemplo: "cuando un bot√≥n sea clickeable").  

---

<img src="Imagenes/PCCOMPONENTES/Ratones-datos/imagen-2.png" width="1600" height="400">  

##  **Clase `PCScraper`**

Esta clase define el **scraper** encargado de extraer informaci√≥n de ratones en **PCComponentes**

### üõ†Ô∏è **¬øC√≥mo funciona?**

1. **Define el `chromedriver.exe` como controlador del navegador** üèé
   - El par√°metro `driver_path="chromedriver.exe"` establece **la ruta del driver de Chrome** para automatizar la navegaci√≥n.  
   - Esto permite que Selenium **controle el navegador y acceda a las p√°ginas de productos**.  

2. **Genera un diccionario con todas las URLs de las p√°ginas** 
   - `self.category_urls` almacena un diccionario donde:  
     - La **clave** es el identificador de la p√°gina (`ratones_p1`, `ratones_p2`, etc.).  
     - El **valor** es la **URL completa** de cada p√°gina de productos.  
   - Usa **un bucle con `range(1, 29)`**, generando autom√°ticamente **las 28 p√°ginas** de resultados.  

3. **Guarda la ruta del controlador y la URL base** 
   - `self.driver_path = driver_path` almacena la ubicaci√≥n del **ChromeDriver**.  
   - `self.base_url = "https://www.pccomponentes.com/"` establece la **URL base de la tienda**.  

---

<img src="Imagenes/PCCOMPONENTES/Ratones-datos/imagen-3.png" width="1600" height="400">  

## **Funci√≥n `start_driver`**

Esta funci√≥n **inicia una nueva sesi√≥n del navegador Chrome** con configuraciones personalizadas para **optimizar el scraping** y **evitar bloqueos**. 

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Crea una configuraci√≥n especial para el navegador** 
   - `chrome_options = Options()` inicializa un objeto para **definir opciones avanzadas de Chrome**.  
   - `chrome_options.add_argument("--start-maximized")` hace que el navegador **se inicie en pantalla completa**, lo que **reduce errores de carga y mejora la visibilidad de los elementos**.  

2Ô∏è‚É£ **Evita la detecci√≥n como bot** 
   - `chrome_options.add_argument("user-agent=Mozilla/...")` cambia el **User-Agent** del navegador.  
   - Simula que la petici√≥n proviene de un usuario real en lugar de un bot, lo que **disminuye la probabilidad de ser bloqueado** por la web.  

3Ô∏è‚É£ **Inicia el navegador con la configuraci√≥n establecida**  
   - `self.driver = webdriver.Chrome(service=Service(self.driver_path), options=chrome_options)`  
   - **Se usa `Service(self.driver_path)`** para asegurarse de que el controlador **ChromeDriver** funcione correctamente.  
   - Todas las opciones configuradas se aplican para garantizar **una navegaci√≥n fluida y sin detecci√≥n**.  

---

<img src="Imagenes/PCCOMPONENTES/Ratones-datos/imagen-4.png" width="600" height="300">  

## **Funci√≥n `close_driver`**  

Esta funci√≥n **cierra el navegador** una vez que el scraping ha terminado, asegurando que los recursos del sistema se liberen correctamente. 

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Cierra la sesi√≥n del navegador** 
   - `self.driver.quit()` **cierra completamente la ventana de Chrome**, finalizando la sesi√≥n de Selenium.  
   - Esto **libera memoria y evita que queden procesos abiertos** innecesariamente.  

---
<img src="Imagenes/PCCOMPONENTES/Ratones-datos/imagen-5.png" width="600" height="300">  

## **Funci√≥n `accept_cookies`**  

Esta funci√≥n **acepta las cookies autom√°ticamente** si el bot√≥n est√° presente en la p√°gina, evitando interrupciones durante el scraping.

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Espera a que el bot√≥n de aceptar cookies est√© disponible** 
   - Usa `WebDriverWait(self.driver, 5).until(EC.element_to_be_clickable((By.ID, 'cookiesAcceptAll')))` para **esperar hasta 5 segundos** a que el bot√≥n sea clickeable.  

2Ô∏è‚É£ **Hace clic en el bot√≥n de aceptaci√≥n** 
   - Si el bot√≥n se encuentra, `accept_button.click()` lo presiona autom√°ticamente.  
   - `time.sleep(2)` agrega una peque√±a pausa de **2 segundos** para asegurar que el clic se registre correctamente.  

3Ô∏è‚É£ **Manejo de errores** 
   - Si el bot√≥n **no aparece o las cookies ya fueron aceptadas**, **se captura la excepci√≥n**.  
   - Muestra el mensaje `"üîπ No se encontr√≥ el bot√≥n de cookies o ya fueron aceptadas."`, permitiendo que el c√≥digo **siga ejecut√°ndose sin problemas**.  

---

<img src="Imagenes/PCCOMPONENTES/Ratones-datos/imagen-6.png" width="600" height="300">  

## **Funci√≥n `generate_url`**  

Esta funci√≥n **genera una URL v√°lida** para un producto a partir de su nombre, asegur√°ndose de que tenga el formato adecuado para ser usada en la web. 

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Elimina caracteres especiales y acentos** 
   - `unidecode.unidecode(product_name)` convierte caracteres con acentos (ej. `C√°mara`) a su equivalente sin acento (`Camara`).  
   - Esto **garantiza compatibilidad con URLs**, evitando caracteres inv√°lidos.  

2Ô∏è‚É£ **Filtra caracteres no deseados** 
   - `re.sub(r'[^a-zA-Z0-9\s]', '', clean_name)` elimina **todo lo que no sea letras, n√∫meros o espacios**.  
   - As√≠ se evita que caracteres especiales rompan la URL.  

3Ô∏è‚É£ **Convierte el texto a min√∫sculas y reemplaza espacios**   
   - `clean_name.lower()` transforma el texto a **min√∫sculas** para mantener consistencia.  
   - `.replace(" ", "-")` reemplaza los espacios por guiones (`-`), que son est√°ndar en URLs.  

4Ô∏è‚É£ **Genera la URL final** 
   - `return f"{self.base_url}{clean_name}"` une la **URL base** con el nombre limpio del producto.  
   - Por ejemplo, si `product_name = "Rat√≥n √ìptico Gamer"`, la salida ser√°:  
     ```
     https://www.pccomponentes.com/raton-optico-gamer
     ```  
--- 


<img src="Imagenes/PCCOMPONENTES/Ratones-datos/imagen-7.png" width="2000" height="1000">  

## **Funci√≥n `scrape_category`**  

Esta funci√≥n **extrae los datos de una sola p√°gina de una categor√≠a espec√≠fica** dentro de la tienda online. 

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Inicia el navegador** 
   - `self.start_driver()` abre una nueva ventana de Chrome para realizar el scraping.  
   - Se asegura de que el navegador **est√© configurado correctamente** antes de comenzar.  

2Ô∏è‚É£ **Carga la p√°gina de la categor√≠a**  
   - `self.driver.get(category_url)` accede a la URL de la categor√≠a que se quiere analizar.  
   - Aqu√≠ es donde estar√°n **todos los productos de esa categor√≠a**.  

3Ô∏è‚É£ **Acepta las cookies si es necesario**  
   - `self.accept_cookies()` busca y **hace clic en el bot√≥n de aceptar cookies** si aparece.  
   - Evita que este aviso bloquee la ejecuci√≥n del scraper.  

### **Esperando la carga de productos en `scrape_category`**  

Esta parte de la funci√≥n **espera a que los productos de la categor√≠a se carguen completamente** antes de extraerlos. 

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Espera a que aparezcan los productos** ‚è≥  
   - `WebDriverWait(self.driver, 15).until(EC.presence_of_element_located(...))`  
   - **Se espera hasta 15 segundos** para que al menos un producto cargue correctamente.  
   - Busca un `h3.product-card__title`, que es el **t√≠tulo del producto** en la p√°gina.  

2Ô∏è‚É£ **A√±ade una pausa extra** 
   - `time.sleep(5)` da **5 segundos adicionales** para asegurarse de que **todos los elementos de la p√°gina est√©n listos**.  
   - Esto ayuda a evitar errores si el contenido se carga lentamente.  

3Ô∏è‚É£ **Extrae la lista de productos** 
   - `products = self.driver.find_elements(By.CSS_SELECTOR, "div.product-card")`  
   - Encuentra **todos los productos** en la p√°gina, cada uno dentro de un `div.product-card`.  
   - Guarda estos elementos en la lista `products`.  

4Ô∏è‚É£ **Prepara la lista de datos**  
   - `product_data = []` crea una lista vac√≠a donde **se guardar√° la informaci√≥n de cada producto**.  
   - En los siguientes pasos, se extraer√°n detalles como **nombre, precio, valoraciones, etc.** y se guardar√°n aqu√≠.  

### **Extracci√≥n de datos de los productos**  

Esta parte de la funci√≥n **recorre todos los productos de la categor√≠a** y extrae informaci√≥n clave como el **nombre, URL y precio**. üè∑Ô∏èüí∞  

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Recorre todos los productos encontrados**   
   - `for product in products:` itera sobre **cada producto en la lista `products`**.  
   - Cada **`product`** representa un art√≠culo en la p√°gina.  

2Ô∏è‚É£ **Extrae el nombre del producto** 
   - `name = product.find_element(By.CSS_SELECTOR, "h3.product-card__title").text.strip()`  
   - Busca el **t√≠tulo del producto** dentro de la tarjeta (`product-card`).  
   - Usa `.strip()` para **eliminar espacios extra** antes y despu√©s del texto.  
   - Si el t√≠tulo **no est√° disponible**, asigna `"No disponible"`.  

3Ô∏è‚É£ **Genera la URL del producto autom√°ticamente**  
   - `url = self.generate_url(name)`  
   - Llama a la funci√≥n `generate_url(name)` para **crear una URL v√°lida basada en el nombre**.  
   - Reemplaza espacios y caracteres especiales para que la URL sea compatible con la web.  

4Ô∏è‚É£ **Extrae el precio del producto** 
   - `price = product.find_element(By.CSS_SELECTOR, "span[data-e2e='price-card']").text.strip()`  
   - Busca el precio dentro de un `span` con el atributo `data-e2e='price-card'`.  
   - Si el precio **no est√° disponible**, asigna `"No disponible"`. 

### ‚≠ê **Extracci√≥n de Valoraciones y Opiniones**  

Esta parte de la funci√≥n **extrae la valoraci√≥n (rating) y el n√∫mero de opiniones** de cada producto, asegur√°ndose de que la informaci√≥n se capture correctamente. 

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Inicializa valores predeterminados** 
   - `rating = "No disponible"` y `opinions = "0"` aseguran que si no se encuentra informaci√≥n, **no haya valores vac√≠os**.  

2Ô∏è‚É£ **Verifica si el producto tiene valoraciones**  
   - `rating_container = product.find_element(By.CSS_SELECTOR, ".product-card__rating-container")`  
   - Busca el **contenedor de valoraciones** dentro de la tarjeta del producto.  
   - Si no existe, el c√≥digo **contin√∫a sin errores**.  

3Ô∏è‚É£ **Extrae la valoraci√≥n num√©rica del producto**   
   - Se buscan `span` dentro del `rating_container`.  
   - Se analiza cada `span.text` para detectar patrones como `4.5/5` o `4,5/5`.  
   - Usa `re.search(r'[0-9][.,][0-9]/[0-9]', text)` para identificar formatos v√°lidos.  
   - Si se encuentra, se guarda en `rating` y se detiene la b√∫squeda.  

4Ô∏è‚É£ **Extrae el n√∫mero de opiniones** 
   - Busca palabras clave como `"opini√≥n"` dentro del contenedor de valoraciones.  
   - Extrae el primer n√∫mero encontrado (`\d+`) y lo asigna a `opinions`.  
   - Si no hay opiniones visibles, el valor **permanece en "0"**.  

5Ô∏è‚É£ **Manejo de errores** 
   - Si el contenedor de valoraciones **no existe**, el c√≥digo **sigue sin fallar**.  
   - Se usa `try-except` en varios niveles para **evitar que errores en una parte bloqueen la ejecuci√≥n completa**.  

### **Almacenamiento de Datos y Manejo de Errores**  

Esta √∫ltima parte de la funci√≥n **guarda los datos extra√≠dos en una lista y maneja posibles errores**, asegurando que el scraping se complete correctamente. 

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Guarda los datos en una lista** 
   - `product_data.append([name, price, rating, opinions, url])`  
   - **Cada producto** se almacena como una lista con los siguientes valores:  
     - üìå `name` ‚Üí Nombre del producto.  
     - üí≤ `price` ‚Üí Precio actual.  
     - ‚≠ê `rating` ‚Üí Valoraci√≥n promedio.  
     - üí¨ `opinions` ‚Üí N√∫mero de opiniones.  
     - üîó `url` ‚Üí Enlace al producto.  
   - Esto permite que todos los datos queden organizados y listos para su uso posterior (ej. guardado en CSV o base de datos).  

2Ô∏è‚É£ **Manejo de errores en productos individuales**   
   - `except Exception as e:` captura cualquier **error durante la extracci√≥n de un producto**.  
   - Si un producto falla, muestra el mensaje `‚ö† Error extrayendo datos de un producto: {e}`.  
   - **El resto de los productos se siguen procesando sin interrupciones**.  

3Ô∏è‚É£ **Devuelve la lista de productos extra√≠dos**   
   - `return product_data` devuelve **todos los datos recopilados** en la categor√≠a actual.  
   - Si no se encuentran productos, se devuelve una **lista vac√≠a** `[]`.  

4Ô∏è‚É£ **Manejo de errores en toda la categor√≠a**   
   - Si hay un error **en toda la p√°gina**, lo captura `except Exception as e:`.  
   - Muestra `‚ùå Error al obtener los productos de {category_url}: {e}` para indicar qu√© fall√≥.  
   - En este caso, tambi√©n devuelve `[]`, asegurando que el scraper no se detenga completamente.  

5Ô∏è‚É£ **Cierra el navegador al finalizar**   
   - `finally: self.close_driver()`  
   - **Se cierra el navegador en todos los casos**, ya sea que la extracci√≥n haya sido exitosa o haya ocurrido un error.  
   - Esto evita **consumo innecesario de recursos** y posibles bloqueos en futuras ejecuciones.  

---

<img src="Imagenes/PCCOMPONENTES/Ratones-datos/imagen-8.png" width="1200" height="400">  

### üìÇ **Funci√≥n `scrape_all_categories`**  

Esta funci√≥n **recorre todas las p√°ginas de productos de la categor√≠a y guarda los datos en archivos CSV separados**. 

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Recorre todas las categor√≠as disponibles** 
   - `for category_name, category_url in self.category_urls.items():`  
   - Itera sobre el **diccionario `self.category_urls`**, que contiene los nombres y URLs de cada categor√≠a.  

2Ô∏è‚É£ **Llama a la funci√≥n `scrape_category` para extraer los productos**  
   - `product_data = self.scrape_category(category_name, category_url)`  
   - Llama a la funci√≥n `scrape_category` para **extraer los productos de la p√°gina actual**.  
   - Si la categor√≠a **tiene productos**, se procede a guardarlos.  

3Ô∏è‚É£ **Guarda los datos en un archivo CSV** 
   - **Genera un nombre de archivo** basado en la categor√≠a:  
     ```python
     csv_filename = f"{category_name}.csv"
     ```
   - **Abre el archivo CSV en modo escritura (`"w"`)**, asegurando que se cree desde cero.  
   - **Escribe los encabezados** en la primera fila:  
     - üìå `Nombre del Producto`  
     - üí≤ `Precio`  
     - ‚≠ê `Valoraciones`  
     - üí¨ `Opiniones`  
     - üîó `URL`  
   - **Escribe los datos de los productos** fila por fila con `writer.writerows(product_data)`.  

4Ô∏è‚É£ **Muestra un mensaje de √©xito**  
   - Indica cu√°ntos productos se han guardado y en qu√© archivo CSV.  
   - Ejemplo de salida:  
     ```
     ‚úÖ Se han guardado 50 productos en 'ratones_p1.csv'.
     ```

5Ô∏è‚É£ **Espera 5 segundos antes de continuar con la siguiente p√°gina** 
   - `time.sleep(5)`  
   - **Evita que el scraper sea detectado como bot**, simulando un comportamiento humano.  

---


<img src="Imagenes/PCCOMPONENTES/Ratones-datos/imagen-9.png" width="600" height="300">  


## **Ejecuci√≥n del Scraper**  

Este bloque de c√≥digo **pone en marcha el scraper**, iniciando el proceso de extracci√≥n de datos de todas las categor√≠as. 

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Verifica que el script se est√° ejecutando directamente**  
   - `if __name__ == "__main__":`  
   - Este condicional asegura que el c√≥digo **solo se ejecute si el script es ejecutado directamente**, y **no si es importado como m√≥dulo** en otro archivo.  

2Ô∏è‚É£ **Crea una instancia del scraper** 
   - `scraper = PCScraper()`  
   - Se inicializa un objeto de la clase `PCScraper`, que contiene toda la l√≥gica de scraping.  
   - Aqu√≠ se configuran **las URLs de las categor√≠as y las opciones del navegador**.  

3Ô∏è‚É£ **Ejecuta el scraping de todas las categor√≠as**   
   - `scraper.scrape_all_categories()`  
   - Llama a la funci√≥n que **recorre todas las p√°ginas de productos y guarda los datos en archivos CSV**.  
   - **Cada categor√≠a se procesa de manera independiente**, asegurando que todas las p√°ginas sean analizadas.  

---

# üéØ **Conclusi√≥n del Apartado de Obtenci√≥n de Datos**  

Hemos completado con √©xito el proceso de **extracci√≥n de datos** con un scraper totalmente funcional. 

## ‚úÖ **¬øQu√© hemos logrado?**  

1Ô∏è‚É£ **Automatizaci√≥n completa** 
   - Desde **abrir el navegador** hasta **guardar los datos en archivos CSV**.  

2Ô∏è‚É£ **Extracci√≥n eficiente**   
   - Obtenci√≥n de **nombre del producto, precio, valoraciones, opiniones y URL**.  

3Ô∏è‚É£ **Manejo de errores inteligente**   
   - **El scraper sigue funcionando** incluso si algunos datos no est√°n disponibles.  

4Ô∏è‚É£ **Optimizaci√≥n anti-detecci√≥n**   
   - **User-Agent personalizado** para evitar bloqueos.  
   - **Pausas estrat√©gicas** para simular un usuario real.  

5Ô∏è‚É£ **Datos listos para an√°lisis**   
   - Todos los productos se han almacenado en **archivos CSV organizados**.  
   - **Perfecto para su visualizaci√≥n en Power BI u otras herramientas de an√°lisis**.  

---

## 3. Limpieza de datos (eliminaci√≥n de nulos y datos err√≥neos, etc.). Descripci√≥n de los datos. Se debe dar una descripci√≥n completa de los datos indicando qu√© significa cada uno de los atributos.

# üßπ **Limpieza de Datos**  

El proceso de limpieza de datos en este proyecto es **sencillo pero esencial** para garantizar que la informaci√≥n extra√≠da sea **relevante y precisa**.  

---

## üéØ **¬øQu√© limpiaremos?**  

üîπ **Im√°genes duplicadas y no relevantes** 
   - Se han detectado **im√°genes promocionales** que **se repiten en el mismo orden** para ratones, teclados y port√°tiles.  
   - Estas im√°genes **usan la misma clase (`class`) que las im√°genes de productos**, por lo que se han incluido accidentalmente en la extracci√≥n.  

üîπ **Filtrado de im√°genes seg√∫n patrones**   
   - Dado que **las im√°genes promocionales siguen un patr√≥n espec√≠fico**, se pueden **identificar y eliminar f√°cilmente**.  
   - Aunque el c√≥digo var√≠a ligeramente entre categor√≠as (**ratones, teclados, port√°tiles**), la l√≥gica **es la misma** en todos los casos.  

---

## üõ†Ô∏è **¬øC√≥mo se hace la limpieza?**  

1Ô∏è‚É£ **Identificaci√≥n de im√°genes no deseadas**  
   - Se analizan **los patrones de repetici√≥n** en las im√°genes extra√≠das.  
   - Se detectan aquellas que corresponden a **banners publicitarios o contenido promocional**.  

2Ô∏è‚É£ **Filtrado autom√°tico en el c√≥digo**  
   - Se implementa una l√≥gica que **excluye las im√°genes** bas√°ndose en su posici√≥n o en atributos espec√≠ficos.  
   - Si una imagen **coincide con el patr√≥n promocional**, **se descarta antes de almacenarla**.  

3Ô∏è‚É£ **Mantenimiento de la calidad de datos**  
   - Solo se conservan **im√°genes relevantes de los productos reales**.  
   - Se evita la presencia de contenido duplicado en la base de datos.  

---

## üî• **¬øPor qu√© es importante este proceso?**  

‚úÖ **Elimina im√°genes irrelevantes**, manteniendo el dataset limpio.  
‚úÖ **Evita duplicados**, mejorando la calidad de los datos.    

---

<img src="Imagenes/PCCOMPONENTES/Limpieza de datos/imagen-1.png" width="800" height="400">  


# üóëÔ∏è **Eliminaci√≥n de Im√°genes No Deseadas**  

Este c√≥digo **elimina im√°genes repetidas o no relevantes** dentro de la carpeta de port√°tiles, asegurando que solo se mantengan las im√°genes √∫tiles. 

---

## üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Define la carpeta principal** 
   - `main_folder = "imagenes_portatiles"`  
   - Aqu√≠ es donde se encuentran las subcarpetas con las im√°genes almacenadas.  

2Ô∏è‚É£ **Lista de archivos a eliminar**  
   - `files_to_delete = {"image"}`  
   - Se crea un **conjunto** con los nombres de archivos que deben ser eliminados.  
   - En este caso, cualquier archivo llamado `"image"` ser√° **borrado autom√°ticamente**.  

3Ô∏è‚É£ **Recorre todas las subcarpetas**   
   - Usa `os.walk(main_folder)` para **iterar sobre cada subcarpeta** dentro de `imagenes_portatiles`.  
   - En cada subcarpeta, **verifica los archivos almacenados**.  

4Ô∏è‚É£ **Elimina archivos innecesarios**   
   - Si un archivo coincide con un nombre en `files_to_delete`, se borra usando `os.remove(file_path)`.  
   - Se muestra un mensaje confirmando cada eliminaci√≥n con `print(f"Eliminado: {file_path}")`.  

5Ô∏è‚É£ **Manejo de errores** 
   - Si ocurre un error al eliminar un archivo, se captura y muestra un mensaje de advertencia:  
     ```
     Error al eliminar <ruta_del_archivo>: <detalle_del_error>
     ```  
   - Esto evita que el c√≥digo se detenga si hay problemas con permisos o archivos inexistentes.  

6Ô∏è‚É£ **Finaliza el proceso**   
   - Al terminar, imprime `"Proceso completado."` indicando que la limpieza ha sido exitosa.  

üìå **Con esta limpieza, garantizamos que los datos extra√≠dos sean √∫tiles y sin ruido.**  

# üîÑ **ETL en AWS Glue: Unificaci√≥n y Transformaci√≥n de Datos**  

Ahora que los datos est√°n listos, realizaremos un **proceso ETL (Extract, Transform, Load)** en **AWS Glue** para unir toda la informaci√≥n y crear una nueva columna.  

---

## üõ†Ô∏è **¬øQu√© es una ETL?**  

üîπ **ETL** significa **Extract, Transform, Load** (**Extraer, Transformar y Cargar**).  
üîπ Es un proceso fundamental en **ingenier√≠a de datos** que permite:  
   1. **Extract (Extracci√≥n):** Obtener datos desde m√∫ltiples fuentes (**CSV, bases de datos, APIs, etc.**).  
   2. **Transform (Transformaci√≥n):** Limpiar, unir, modificar y generar nuevas columnas.  
   3. **Load (Carga):** Guardar los datos transformados en un formato optimizado (**S3, Redshift, Power BI, etc.**).  

---

## **ETL en AWS Glue con PySpark y Modo Visual**  

AWS Glue es un servicio **serverless** que permite **automatizar procesos ETL** con dos enfoques principales:  

üîπ **Modo Visual (Similar a Spoon)** 
   - Ofrece una **interfaz gr√°fica** donde se pueden **arrastrar y conectar componentes**.  
   - Es √∫til para tareas b√°sicas, pero **tiene limitaciones en personalizaci√≥n**.  

üîπ **Modo C√≥digo (PySpark)** 
   - Permite **personalizar transformaciones avanzadas** y trabajar con grandes vol√∫menes de datos.  
   - Es **m√°s flexible**, ideal para procesos complejos.  

üîπ **Nuestro Enfoque**  
   - **Trabajaremos en el modo visual hasta cierto punto** para facilitar la manipulaci√≥n de datos.  
   - Luego, **cambiaremos a c√≥digo (PySpark)** cuando necesitemos **guardar m√∫ltiples archivos en uno solo**.  

---

## üéØ **Objetivo de nuestra ETL**  

‚úÖ **Unificar toda la informaci√≥n** proveniente de diferentes archivos CSV.  
‚úÖ **Crear una nueva columna** con datos transformados.  
‚úÖ **Optimizar los datos y consolidarlos en un √∫nico fichero** para su an√°lisis en Power BI.  

---


<img src="Imagenes/ETL/imagen-1.png" width="1600" height="800">  

# üõ†Ô∏è **Dise√±o de la ETL en AWS Glue (Modo Visual)**  

La imagen muestra la configuraci√≥n de una ETL en **AWS Glue Studio**, utilizando el **modo visual** para procesar los datos de un bucket de **Amazon S3**, transformarlos con **SQL Query** y almacenarlos nuevamente en **S3**.  

---

## üîç **Estructura de la ETL en la imagen**  

1Ô∏è‚É£ **Fuente de Datos: Amazon S3**   
   - Se carga la informaci√≥n desde un **bucket de S3**.  
   - Esto puede ser un conjunto de archivos **CSV, JSON u otro formato compatible**.  

2Ô∏è‚É£ **Transformaci√≥n: SQL Query**  
   - Se aplica una transformaci√≥n utilizando **SQL**.  
   - Aqu√≠ se pueden **filtrar, modificar columnas o unir datasets**.  

3Ô∏è‚É£ **Destino de Datos: Amazon S3**   
   - Los datos transformados se guardan en otro **bucket de S3**.  
   - En este caso, se ha seleccionado el formato **CSV** y se ha definido una ubicaci√≥n espec√≠fica.  

---

## **C√≥mo deber√≠a haber sido la ETL**  

üîπ **El dise√±o visual permite realizar muchas operaciones de transformaci√≥n**, pero tiene **limitaciones** cuando se requiere personalizaci√≥n avanzada.  

üîπ En nuestro caso, el **modo visual es √∫til hasta cierto punto**, pero **necesitamos cambiar a c√≥digo (PySpark)** para:  
   - **Unir m√∫ltiples archivos en uno solo**.  
   - **Aplicar transformaciones m√°s flexibles** que no se pueden realizar con la interfaz visual.  
   - **Definir bien el formato** en el almacenamiento del S3.  

üìå **A continuaci√≥n, explicaremos el c√≥digo que reemplazar√° esta configuraci√≥n para lograr la ETL completa en AWS Glue.** üöÄ  

<img src="Imagenes/ETL/imagen-2.png" width="600" height="300">  

## üöÄ **Importaciones necesarias**  

Este c√≥digo **prepara el entorno** para ejecutar una tarea en **AWS Glue**, una herramienta que ayuda a procesar datos de manera eficiente en la nube ‚òÅÔ∏èüìä.  

### üõ†Ô∏è **¬øQu√© hace cada l√≠nea?**  

1. **Importa librer√≠as esenciales**   
   - `sys`: Permite interactuar con el sistema operativo y gestionar argumentos.  
   - `awsglue.transforms`: Contiene funciones para transformar datos dentro de AWS Glue.  
   - `awsglue.utils.getResolvedOptions`: Sirve para obtener par√°metros que se pasan al script desde AWS Glue.  

2. **Configura el entorno de ejecuci√≥n en Spark**   
   - `SparkContext`: Crea el contexto de Spark, que es el motor que procesar√° los datos en paralelo.  
   - `GlueContext`: Un entorno especial de AWS Glue que facilita la integraci√≥n con otros servicios de AWS.  
   - `Job`: Permite definir una **tarea en AWS Glue**, facilitando su ejecuci√≥n y seguimiento.  

3. **Eval√∫a la calidad de los datos** 
   - `EvaluateDataQuality`: Un m√≥dulo de AWS Glue que **verifica si los datos son correctos** y cumplen con ciertos est√°ndares antes de seguir proces√°ndolos.  

4. **Usa `DynamicFrame` en lugar de DataFrames tradicionales**   
   - `DynamicFrame`: Una versi√≥n mejorada de los DataFrames de Spark, dise√±ada para trabajar con datos en AWS Glue.  
   - Permite **manejar datos semiestructurados** y **hacer transformaciones f√°cilmente** sin necesidad de definir un esquema fijo.  

---

<img src="Imagenes/ETL/imagen-3.png" width="600" height="300">     

## **Funci√≥n `sparkSqlQuery`**  

Esta funci√≥n permite **ejecutar consultas SQL en datos almacenados en AWS Glue**. Es √∫til para **transformar y analizar los datos** sin necesidad de escribir c√≥digo complejo, solo utilizando SQL. üìäüí°  

### **¬øC√≥mo funciona?**  

1. **Recibe los siguientes par√°metros** 
   - üèóÔ∏è `glueContext`: Proporciona acceso a AWS Glue y su integraci√≥n con Spark.  
   - üìú `query`: La consulta SQL que se aplicar√° a los datos.  
   - üîÑ `mapping`: Un diccionario donde **las claves son nombres de tablas temporales** y **los valores son `DynamicFrame` con los datos a procesar**.  
   - üîç `transformation_ctx`: Identificador que permite rastrear la transformaci√≥n dentro de Glue.  

2. **Convierte los `DynamicFrame` en tablas temporales**  
   - üìå **Recorre cada elemento en `mapping`**, extrayendo el alias (nombre de la tabla) y los datos (`DynamicFrame`).  
   - üóÇÔ∏è **Transforma cada `DynamicFrame` en un `DataFrame` de Spark** para poder consultarlo con SQL.  
   - üè∑Ô∏è **Crea una tabla temporal** (`createOrReplaceTempView(alias)`) para que la consulta SQL pueda referirse a ella.  

3. **Ejecuta la consulta SQL sobre los datos**   
   - üìä Utiliza `spark.sql(query)` para aplicar la transformaci√≥n deseada.  

4. **Convierte el resultado en un `DynamicFrame` nuevamente**  
   - ‚ú® `DynamicFrame.fromDF(result, glueContext, transformation_ctx)`:  
     - **Toma los datos procesados** y los **devuelve en formato `DynamicFrame`**.  
     - Esto permite **seguir aplicando transformaciones** en AWS Glue sin problemas.  

---

<img src="Imagenes/ETL/imagen-4.png" width="600" height="300">  

## **Configuraci√≥n del Entorno en AWS Glue**  

Este fragmento de c√≥digo **inicializa el entorno de ejecuci√≥n en AWS Glue**, preparando Spark para procesar datos en la nube. 

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1. **Obtiene los par√°metros del trabajo en Glue** 
   - `args = getResolvedOptions(sys.argv, ['JOB_NAME'])`  
   -  Extrae el nombre del trabajo desde los **argumentos que recibe el script** cuando se ejecuta en AWS Glue.  
   - `JOB_NAME` es el identificador del trabajo dentro de AWS.  

2. **Inicializa Spark**   
   - `sc = SparkContext()`  
   -  Crea el **contexto de Spark**, necesario para ejecutar tareas de procesamiento de datos en paralelo.  

3. **Crea el contexto de AWS Glue** üõ†Ô∏è  
   - `glueContext = GlueContext(sc)`  
   -  Permite que AWS Glue utilice Spark para transformar y manejar datos de forma eficiente.  
   -  **Facilita la conexi√≥n con otras herramientas de AWS** como S3, Redshift o DynamoDB.  

4. **Obtiene la sesi√≥n de Spark**   
   - `spark = glueContext.spark_session`  
   - **Crea una sesi√≥n de Spark**, que es necesaria para ejecutar consultas y manipular datos.  

5. **Inicializa el trabajo en Glue**   
   - `job = Job(glueContext)`  
   -  Crea una **instancia de trabajo en AWS Glue**, permitiendo **controlar su ejecuci√≥n y finalizarlo correctamente**.  
   - `job.init(args['JOB_NAME'], args)`:  
     - **Inicia el trabajo** con el nombre que se obtuvo en el primer paso.  
     - **Permite que AWS Glue rastree y administre el estado del trabajo.**  

---

<img src="Imagenes/ETL/imagen-5.png" width="600" height="300">  

## üìè **Reglas por Defecto para la Calidad de Datos**  

Este fragmento define un **conjunto de reglas b√°sicas** que AWS Glue utilizar√° para **validar la calidad de los datos** antes de procesarlos.   

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1. **Define un conjunto de reglas de calidad de datos** 
   - Se almacena en la variable `DEFAULT_DATA_QUALITY_RULESET`.  
   - Contiene un conjunto de reglas que AWS Glue **aplicar√° autom√°ticamente** a todos los nodos de destino que tengan habilitada la validaci√≥n de calidad de datos.  

2. **Reglas establecidas**   
   - `ColumnCount > 0`  
   -  **Verifica que la tabla tenga al menos una columna**.  
   -  **Evita procesar datos vac√≠os o estructuras incorrectas** que podr√≠an causar errores m√°s adelante.  

---

<img src="Imagenes/ETL/imagen-5.png" width="600" height="300">  

## **Carga de Datos desde Amazon S3**  

Este fragmento de c√≥digo **lee un archivo CSV almacenado en Amazon S3** y lo convierte en un `DynamicFrame` para su procesamiento en AWS Glue.   

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1. **Configura la lectura de datos desde S3** 
   - `AmazonS3_node1740744770173 = glueContext.create_dynamic_frame.from_options(...)`  
   - **Convierte los datos en un `DynamicFrame`**, que es un formato optimizado para AWS Glue.  

2. **Define las opciones de formato**   
   - `format_options={"quoteChar": "\"", "withHeader": True, "separator": ","}`  
   - `quoteChar`: Usa `"` para manejar valores entre comillas.  
   - `withHeader`: Indica que el CSV tiene una fila de encabezados.  
   - `separator`: Especifica que las columnas est√°n separadas por comas (`,`).  

3. **Especifica la fuente de datos**   
   - `connection_type="s3"`: Indica que los datos provienen de un **bucket de Amazon S3**.  
   - `connection_options={"paths": ["s3://inervisionai"], "recurse": True}`  
     - **Ruta del bucket**: `s3://inervisionai`.  
     - `recurse=True`: Permite leer archivos dentro de subcarpetas.  

4. **Asigna un identificador al proceso de transformaci√≥n** üîç  
   - `transformation_ctx="AmazonS3_node1740744770173"`  
   - AWS Glue usa este identificador para rastrear y organizar las transformaciones aplicadas al `DynamicFrame`.  

---

## üñ•Ô∏è **Consulta SQL para Clasificaci√≥n de Marcas**  

Este fragmento de c√≥digo SQL **clasifica los productos seg√∫n su marca** üè∑Ô∏è. La consulta revisa el nombre de cada producto y asigna una marca espec√≠fica si detecta ciertas palabras clave.  

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1. **Selecciona todas las columnas del dataset** üìä  
   - `SELECT *,`  
   - üîÑ Mantiene todos los datos originales del conjunto de datos (`myDataSource`).  

2. **Crea una nueva columna llamada `Marca`** üè∑Ô∏è  
   - `CASE WHEN lower(Nombre del Producto) LIKE '%logitech%' THEN 'Logitech'`  
   - üîç **Convierte el nombre del producto a min√∫sculas** (`lower(Nombre del Producto)`) para evitar problemas con may√∫sculas/min√∫sculas.  
   - üîé Si el nombre del producto contiene `"logitech"`, asigna la marca `"Logitech"`.  
   - üè∑Ô∏è Este mismo proceso se repite para otras marcas como **Razer, Corsair, Asus, MSI, Apple, etc.**  

3. **Si el producto no coincide con ninguna marca conocida** ‚ùì  
   - `ELSE 'Otra'`  
   - üè∑Ô∏è Si el producto **no pertenece a ninguna marca listada**, se le asigna la categor√≠a `"Otra"`.  

4. **Aplica la consulta sobre la fuente de datos** üíæ  
   - `FROM myDataSource;`  
   - üìå `myDataSource` es el nombre de la tabla o `DynamicFrame` que contiene los datos originales.  

---

<img src="Imagenes/ETL/imagen-7.png" width="1400" height="450">  

## üöÄ **Ejecuci√≥n de Consulta SQL y Almacenamiento en Amazon S3**  

Este fragmento de c√≥digo **ejecuta una consulta SQL en los datos, transforma el resultado y lo guarda en un archivo CSV en Amazon S3**. üìä‚òÅÔ∏è  

### üõ†Ô∏è **¬øC√≥mo funciona?**  

1. **Ejecuta la consulta SQL sobre los datos de Amazon S3** üñ•Ô∏è  
   - Se usa la funci√≥n `sparkSqlQuery` para ejecutar la consulta SQL almacenada en `SqlQuery0`.  
   - Los datos provienen de `AmazonS3_node1740744770173` y se asigna un alias (`myDataSource`) para facilitar la consulta.  
   - El resultado de la consulta se guarda en `SQLQuery_node1740745043008`, que ser√° utilizado en los siguientes pasos.  

2. **Convierte el resultado en un `DataFrame` de Spark** üîÑ  
   - Se transforma el `DynamicFrame` resultante de la consulta en un `DataFrame` para poder manipular los datos con mayor facilidad.  

3. **Verifica si hay datos antes de continuar** ‚ùó  
   - Antes de escribir en Amazon S3, se comprueba que el `DataFrame` no est√© vac√≠o.  
   - Si no hay datos, se genera un error (`ValueError`) y el proceso se detiene para evitar archivos vac√≠os en S3.  

4. **Reduce el n√∫mero de particiones para generar un solo archivo CSV** üìÇ  
   - Se reorganizan los datos en una √∫nica partici√≥n para que Spark genere un solo archivo CSV en lugar de m√∫ltiples fragmentos.  

5. **Muestra el esquema y algunas filas para depuraci√≥n** üîç  
   - Se imprime la estructura de las columnas del `DataFrame` para verificar su formato.  
   - Se muestran las primeras cinco filas para comprobar que los datos se han procesado correctamente.  

6. **Guarda el resultado en Amazon S3 en formato CSV** ‚òÅÔ∏è  
   - Se escribe el `DataFrame` en Amazon S3 en formato CSV con sobrescritura activada, lo que significa que si el archivo ya existe, se reemplazar√°.  
   - Se incluye la opci√≥n de encabezados para que el archivo CSV mantenga los nombres de las columnas.  

7. **Confirma que el trabajo ha finalizado** ‚úÖ  
   - Se ejecuta el comando `job.commit()`, lo que indica que el trabajo en AWS Glue ha finalizado correctamente.  

---


## 4. Exploraci√≥n y visualizaci√≥n de los datos. Se realizar√° un estudio de los datos buscando correlaciones, mostrando gr√°ficas de diferente tipolog√≠a, observando si hay valores nulos, etc.


# **Almacenamiento de Im√°genes en CSV**  

Despu√©s de eliminar las im√°genes no deseadas, el siguiente paso es **registrarlas en un archivo CSV**. üìÇüîÑ  

## üéØ **¬øPor qu√© guardar las im√°genes en un CSV?**  

üîπ **Organizaci√≥n** ‚Üí Permite estructurar los datos para su f√°cil an√°lisis.  
üîπ **Accesibilidad** ‚Üí Un CSV es ligero y compatible con m√∫ltiples herramientas de an√°lisis.  

---

 <img src="Imagenes/Limpieza de datos/imagen-2.png" width="600" height="300">  

# üìÇ **Definici√≥n de Carpetas y Lista de Datos**  

Antes de procesar las im√°genes, es necesario **definir las carpetas de origen** y **crear una lista para almacenar la informaci√≥n** extra√≠da. üñºÔ∏èüìä  

---

## üõ†Ô∏è **¬øC√≥mo funciona esta parte del c√≥digo?**  

1Ô∏è‚É£ **Definir las carpetas donde est√°n las im√°genes** üìÇ  
   - `carpetas_principales = ["../../ikea_muebles/sillas"]`  
   - Se establece una lista con **las rutas de las carpetas principales**, donde se encuentran las im√°genes organizadas en subcarpetas.  
   - En este caso, se est√° procesando la carpeta `"sillas"` dentro de `"ikea_muebles"`.  

2Ô∏è‚É£ **Crear una lista para almacenar los datos** üìù  
   - `data = []`  
   - Se inicializa una **lista vac√≠a** que **almacenar√° la informaci√≥n de cada imagen**.  
   - Posteriormente, en esta lista se guardar√°n datos como:  
     - üñºÔ∏è `Nombre del archivo`  
     - üìÇ `Ruta de la imagen`  
     - üî† `Imagen codificada en base64`  

---

<img src="Imagenes/Limpieza de datos/imagen-3.png" width="1200" height="400">  

# **Funci√≥n `procesar_carpeta`**  

Esta funci√≥n **recorre carpetas y subcarpetas**, buscando im√°genes, **convirti√©ndolas a Base64** y almacen√°ndolas en una lista con formato HTML. üìÇüìä  

---

## üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Recorre todas las carpetas y subcarpetas** üîÑ  
   - `os.walk(os.path.abspath(carpeta_raiz))`  
   - Convierte la **ruta relativa en absoluta** para evitar errores.  
   - **Recorre recursivamente** todas las carpetas y subcarpetas dentro de `carpeta_raiz`.  

2Ô∏è‚É£ **Filtra archivos de imagen** üè∑Ô∏è  
   - `if archivo.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):`  
   - **Solo procesa archivos de imagen** con extensiones comunes.  
   - Ignora otros tipos de archivos no relevantes.  

3Ô∏è‚É£ **Convierte la imagen a Base64** üîÑ  
   - `with open(ruta_imagen, "rb") as img_file:`  
   - Abre la imagen en **modo lectura binaria (`rb`)**.  
   - `base64.b64encode(img_file.read()).decode('utf-8')`  
   - **Codifica la imagen en Base64** y la convierte en un **string de texto**.  

4Ô∏è‚É£ **Genera una etiqueta HTML con la imagen en Base64** üñºÔ∏è  
   - `img_html = f'<img src="data:image/png;base64,{base64_str}" width="100"/>'`
   - Se establece un **ancho de `100px`** para previsualizaci√≥n.  

5Ô∏è‚É£ **Agrega la imagen a la lista de datos** üìã  
   - `data.append([img_html])`  
   - Guarda la informaci√≥n en la lista `data`, para su posterior almacenamiento en CSV.  

6Ô∏è‚É£ **Manejo de errores** ‚ö†Ô∏è  
   - Si ocurre alg√∫n problema al procesar una imagen, el error **se muestra en consola** y el programa sigue ejecut√°ndose.  

---

<img src="Imagenes/Limpieza de datos/imagen-4.png" width="600" height="300">  

# üìÑ **Conversi√≥n de Im√°genes a CSV**  

Despu√©s de procesar todas las im√°genes, este c√≥digo **crea un DataFrame y lo guarda en un archivo CSV**, asegurando que est√© listo para el entrenamiento del modelo.

---

## üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Procesa todas las carpetas principales** üìÇ  
   - `for carpeta in carpetas_principales:`  
   - **Recorre cada carpeta** y ejecuta `procesar_carpeta(carpeta)`.  
   - Se almacenan las im√°genes **convertidas a Base64 con formato HTML** en la lista `data`.  

2Ô∏è‚É£ **Crea un DataFrame con los datos** üèóÔ∏è  
   - `df = pd.DataFrame(data, columns=["chair"])`  
   - Se genera un **DataFrame de Pandas** con una columna llamada `"chair"`.  
   - **Cada fila contiene una imagen en formato Base64 con etiqueta HTML**.  

3Ô∏è‚É£ **Guarda el DataFrame en un archivo CSV** üíæ  
   - `csv_path = "chair.csv"` define el nombre del archivo.  
   - `df.to_csv(csv_path, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL, escapechar="\\")`  
     - üîπ **`index=False`** ‚Üí No guarda el √≠ndice del DataFrame.  
     - üîπ **`encoding="utf-8-sig"`** ‚Üí Asegura compatibilidad con **Excel**.  
     - üîπ **`quoting=csv.QUOTE_MINIMAL`** ‚Üí Evita problemas con comillas en los datos.  
     - üîπ **`escapechar="\\ "`** ‚Üí Escapa caracteres especiales para evitar errores en la lectura del CSV.  

4Ô∏è‚É£ **Muestra un mensaje de √©xito** ‚úÖ  
   - `print(f"‚úÖ Archivo CSV guardado correctamente en: {csv_path}")`  
   - Confirma que el archivo se ha guardado **sin errores y listo para su an√°lisis**.  

---
 
<img src="Imagenes/Visualizaci√≥n_de_datos/imagen-1.png" width="600" height="300">

# üì¶ **Importaci√≥n de M√≥dulos para la Prueba y Visualizaci√≥n**  

Antes de verificar el correcto funcionamiento de los datos, necesitamos **importar las librer√≠as necesarias** para **cargar, procesar y visualizar las im√°genes** almacenadas en el CSV.  

---

## üõ†Ô∏è **¬øQu√© hace cada m√≥dulo?**  

1Ô∏è‚É£ **`pandas` ‚Üí Manejo de Datos Tabulares** 
   - `import pandas as pd`  
   - Permite **cargar el CSV** y trabajar con √©l como un **DataFrame**.  
   - Se usar√° para leer y verificar la estructura de los datos.  

2Ô∏è‚É£ **`matplotlib.pyplot` ‚Üí Visualizaci√≥n de Datos** 
   - `import matplotlib.pyplot as plt`  
   - Nos permite **mostrar las im√°genes** contenidas en el CSV.  
   - Se usar√° para graficar y confirmar que las im√°genes se han guardado correctamente.  

3Ô∏è‚É£ **`base64` ‚Üí Decodificaci√≥n de Im√°genes**  
   - `import base64`  
   - Convierte las im√°genes **de Base64 a un formato visualizable**.  
   - Se usar√° para reconstruir las im√°genes almacenadas en el CSV.  

4Ô∏è‚É£ **`BytesIO` ‚Üí Manejo de Archivos en Memoria**   
   - `from io import BytesIO`  
   - Permite trabajar con **im√°genes sin necesidad de guardarlas en disco**.  
   - Se usar√° para cargar im√°genes directamente en memoria antes de visualizarlas.  

5Ô∏è‚É£ **`PIL.Image` ‚Üí Procesamiento de Im√°genes** 
   - `from PIL import Image`  
   - Nos permite **abrir, procesar y mostrar im√°genes** en Python.  
   - Se usar√° para reconstruir las im√°genes en Base64 y mostrarlas en pantalla.  

---

<img src="Imagenes/Visualizaci√≥n_de_datos/imagen-2.png" width="600" height="300">

# üìÑ **Lectura y Extracci√≥n de Im√°genes desde CSV**  

En este paso, **cargamos el archivo CSV y extraemos la primera imagen** almacenada en formato Base64 para verificar su correcta codificaci√≥n. 

---

## üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Carga el archivo CSV**   
   - `df = pd.read_csv('chair.csv')`  
   - **Lee el archivo `chair.csv`** usando Pandas y lo almacena en un **DataFrame**.  
   - Contiene la columna `"chair"` con im√°genes en formato **Base64 dentro de etiquetas HTML**.  

2Ô∏è‚É£ **Obtiene la primera fila del DataFrame**   
   - `first_row = df.iloc[0]`  
   - Usa `.iloc[0]` para **seleccionar la primera fila** del DataFrame.  
   - Se extrae una imagen **para verificar que los datos est√°n bien guardados**.  

3Ô∏è‚É£ **Extrae la imagen en Base64**   
   - `img_base64 = first_row['chair']`  
   - Se accede a la columna `"chair"` de la primera fila.  
   - **Aqu√≠ se encuentra el c√≥digo Base64 dentro de una etiqueta HTML**.  

---


<img src="Imagenes/Visualizaci√≥n_de_datos/imagen-3.png" width="600" height="300">


# **Verificaci√≥n, Decodificaci√≥n y Visualizaci√≥n de la Imagen**  

Este c√≥digo **verifica que la imagen en Base64 est√© en el formato correcto, la decodifica y la muestra en pantalla**.   

---

## üõ†Ô∏è **¬øC√≥mo funciona?**  

1Ô∏è‚É£ **Verifica si la cadena Base64 tiene un prefijo de datos**   
   - `if ',' in img_base64:`  
   - Algunas im√°genes en Base64 **pueden incluir un prefijo**, por ejemplo:  
     ```
     data:image/png;base64,iVBORw...
     ```
   - Si hay una coma `,`, significa que el prefijo est√° presente.  
   - `img_base64.split(',')[1]` extrae **solo la parte Base64**, eliminando `"data:image/png;base64,"`.  

2Ô∏è‚É£ **Asegura que la longitud de la cadena sea un m√∫ltiplo de 4**   
   - `padding = len(img_base64) % 4`  
   - Base64 **debe tener una longitud en m√∫ltiplos de 4**.  
   - Si no lo es, **se agregan los caracteres `=` necesarios** para corregir la cadena.  
   - `img_base64 += '=' * (4 - padding)` **corrige la longitud si es necesario**.  

3Ô∏è‚É£ **Intenta decodificar la imagen** 
   - `img_data = base64.b64decode(img_base64)`  
   - Convierte la cadena **de Base64 a datos binarios de imagen**.  

4Ô∏è‚É£ **Convierte los datos en una imagen visualizable**   
   - `img = Image.open(BytesIO(img_data))`  
   - Usa `BytesIO` para **convertir los datos binarios en una imagen sin guardarla en disco**.  
   - `Image.open()` abre la imagen lista para ser visualizada.  

5Ô∏è‚É£ **Muestra la imagen**   
   - `plt.figure()` ‚Üí Crea una nueva figura para la imagen.  
   - `plt.imshow(img)` ‚Üí Muestra la imagen decodificada.  
   - `plt.axis('off')` ‚Üí Oculta los ejes para mejorar la visualizaci√≥n.  
   - `plt.show()` ‚Üí Muestra la imagen en pantalla.  

6Ô∏è‚É£ **Manejo de errores**   
   - Si ocurre un error en la decodificaci√≥n, se captura con `except Exception as e`.  
   - Se imprime un mensaje de error con `print(f"Error al decodificar la imagen: {e}")`.  

---

<img src="Imagenes/Visualizaci√≥n_de_datos/imagen-4.png" width="1200" height="600"> 

# üñ±Ô∏è **Verificaci√≥n de Datos de Ratones**  

Ahora que hemos extra√≠do y almacenado los datos, **vamos a verificar su integridad** antes de proceder con la visualizaci√≥n en Power BI. üìä  

---

## üõ†Ô∏è **¬øC√≥mo funciona esta prueba?**  

1Ô∏è‚É£ **Carga el archivo CSV** 
   - `df_datos = pd.read_csv('/content/ratones_p1.csv')`  
   - Usa Pandas para **leer los datos almacenados en el archivo CSV**.  

2Ô∏è‚É£ **Muestra las primeras filas** 
   - `print(df_datos.head())`  
   - Permite visualizar las primeras 5 filas del dataset para asegurarnos de que los datos est√°n bien organizados.  

3Ô∏è‚É£ **Cuenta los valores nulos por columna** 
   - `print(df_datos.isna().sum())`  
   - Muestra **cu√°ntos valores nulos hay en cada columna**, ayudando a identificar posibles problemas en los datos.  

---

## üìä **Visualizaci√≥n de Datos en Power BI**  

Una vez que hemos procesado y comprobado de que no tiene **null** en ninguna fila, el siguiente paso es **visualizarlos en Power BI** para analizarlos de manera m√°s intuitiva. üìà‚ú®  

<img src="Imagenes/inervision.png">

Para visualizar el Power Bi de manera interactiva [aqui](https://app.powerbi.com/links/GojVNB_DqB?ctid=e0793d39-0939-496d-b129-198edd916feb&pbi_source=linkShare). **Nota** : Es importante tener cuenta de power BI para poder acceder. 

## 5. Preparaci√≥n de los datos para los algoritmos de Machine Learning. Se deben tratar los datos (limpiando, escalando, separando y todo lo que sea necesario) de tal forma que queden listos para entrenar el modelo.
### 5.1 Proceso de Fine-Tuning con YOLOv5

El objetivo del fine-tuning es adaptar un modelo preentrenado de YOLOv5 (yolov5nu.pt) a nuestro dataset, mejorando su capacidad de detecci√≥n en nuestro caso de uso espec√≠fico.

### üìÇ 5.1.1 Preparaci√≥n del Dataset
Para entrenar el modelo, primero preparamos los datos siguiendo los pasos detallados a continuaci√≥n:

1Ô∏è‚É£ Obtenci√≥n del Dataset

Realizamos scraping de im√°genes, almacen√°ndolas en un archivo CSV en formato base64.

üìå Ejemplo de archivo:
![image](https://github.com/user-attachments/assets/6803008a-0ca4-4ad6-830b-18efb486ba31)

---

2Ô∏è‚É£ Conversi√≥n de Im√°genes

Como las im√°genes estaban almacenadas en formato base64, era necesario decodificarlas para poder usarlas en el entrenamiento.

Utilizamos el siguiente script en Python para convertir las im√°genes de base64 a `.jpg`:

![image](https://github.com/user-attachments/assets/58af828c-a9dc-40a4-9005-2d6db3b7bb56)

---

3Ô∏è‚É£ Etiquetado de Im√°genes

Para entrenar un modelo de detecci√≥n de objetos, cada imagen necesita etiquetas con las coordenadas de los objetos. Utilizamos **Roboflow**, una plataforma que permite:

- ‚úÖ Subir im√°genes.
- ‚úÖ Etiquetar im√°genes manualmente o autom√°ticamente con herramientas de anotaci√≥n.
- ‚úÖ Convertir el dataset a formatos compatibles con modelos de detecci√≥n como YOLOv5.
- ‚úÖ Dividir los datos en conjuntos de entrenamiento, validaci√≥n y prueba.

Nuestro objetivo fue **etiquetar autom√°ticamente** im√°genes del dataset para detectar objetos de inter√©s y exportarlas en formato YOLOv5.

üîπ Creaci√≥n de un Proyecto en Roboflow

- Asignamos un nombre al proyecto, por ejemplo: cupboard_detection.
- Seleccionamos el tipo de modelo: Object Detection (YOLOv5, COCO, etc.)

![image](https://github.com/user-attachments/assets/6ad1ed78-265d-44b6-8ecb-ff13b256031b)
  

üîπ Subida de Im√°genes al Proyecto

![image](https://github.com/user-attachments/assets/5c00338e-e366-49b7-b870-caab13064313)

üîπ Etiquetado Autom√°tico de Objetos

Dado que Roboflow cuenta con herramientas de etiquetado autom√°tico, utilizamos esta opci√≥n para generar anotaciones sin intervenci√≥n manual.

![image](https://github.com/user-attachments/assets/65260858-f3dd-4f6e-b690-78e095ef2e20)

Si bien el etiquetado autom√°tico es preciso, verificamos que las anotaciones fueran correctas.

![image](https://github.com/user-attachments/assets/b50e683b-e12b-4f62-906e-a672ed6310d8)

En caso de errores, ajustamos manualmente las etiquetas antes de continuar para completar el proceso.

![image](https://github.com/user-attachments/assets/ebea7738-f6de-4def-853f-dd361ac78e29)

Despu√©s de la comprobaci√≥n a√±adimos las etiquetas aprobadas.

![image](https://github.com/user-attachments/assets/faf2c310-4d9d-47ba-8a08-9e818f91ae73)



üîπ Exportaci√≥n del Dataset en Formato YOLOv5

Para utilizar las im√°genes etiquetadas en el entrenamiento del modelo, exportamos el dataset en formato YOLOv5.

Roboflow nos permite dividir el dataset en tres subconjuntos:
- 80% para entrenamiento (train)
- 10% para validaci√≥n (valid)
- 10% para prueba (test)

![image](https://github.com/user-attachments/assets/9a021657-f59a-4b1c-bac5-f258790e0bf2)

En la secci√≥n de exportaci√≥n, seleccionamos YOLOv5 como formato de salida y descargamos un archivo ZIP.

![image](https://github.com/user-attachments/assets/a4fd3ee4-1efb-4581-8b08-c9d18a9aefb9)

El archivo ZIP tiene la siguiente estructura:

```
/dataset
‚îÇ‚îÄ‚îÄ test/    # 10% de im√°genes para prueba
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ labels/   
‚îÇ
‚îÇ‚îÄ‚îÄ train/   # 80% de im√°genes para entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ labels/
|
‚îÇ‚îÄ‚îÄ valid/   # 10% de im√°genes para validaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ labels/
‚îÇ
‚îÇ‚îÄ‚îÄ data.yaml    # Archivo de configuraci√≥n del dataset
```

En el directorio `labels` obtenemos archivos .txt con las coordenadas de los objetos.


### üõ† 5.1.2 Entrenamiento del Modelo

Para el entrenamiento utilizamos el modelo preentrenado **yolov5nu.pt**. Ejecutamos el proceso en Google Colab con GPU habilitada para acelerar el c√≥mputo. Clonamos el repositorio de YOLOv5. Usamos los siguientes par√°metros en el script de entrenamiento:

```python
!python train.py \
  --weights /content/drive/MyDrive/Models/yolov5nu.pt \
  --data /content/drive/MyDrive/YOLO_Dataset/data.yaml \
  --epochs 50 \
  --batch-size 16 \
  --imgsz 640 \
  --optimizer SGD \
  --device 0
```

Sin embargo, durante la ejecuci√≥n del entrenamiento encontramos errores relacionados con la configuraci√≥n de los anchors en el modelo. El siguiente es un ejemplo de los errores que recibimos:

```
RuntimeError: shape '[3, -1, 2]' is invalid for input of size 3
```

Este error sugiere que los anchors definidos en el modelo no se ajustaban correctamente al n√∫mero de clases u otras dimensiones esperadas. Intentamos modificar la configuraci√≥n, pero el problema persisti√≥.

---

‚ö†Ô∏è Problemas Encontrados y Conclusi√≥n

No logramos completar el fine-tuning debido a erroresencontrados. Las posibles causas incluyen:

1. **Incompatibilidad en los anchors**: La configuraci√≥n de los anchors puede no haber sido adecuada para nuestro dataset. 
2. **Formato incorrecto en el archivo data.yaml**: Es posible que las clases o los par√°metros en el archivo no estuvieran correctamente definidos.
3. **Modelo preentrenado incompatible**: Puede que el modelo **yolov5nu.pt** no estuviera configurado correctamente para ser reutilizado con nuevos datos.

Para solucionar estos problemas, proponemos:
- Revisar el formato de **data.yaml** y asegurarnos de que est√° bien definido.
- Ajustar los anchors manualmente o permitir que YOLO los recalibre autom√°ticamente.
- Probar con otro modelo preentrenado de YOLOv5 para verificar compatibilidad.

A pesar de las dificultades, este proceso nos permiti√≥ comprender mejor el flujo de trabajo de YOLOv5 y los retos asociados a la personalizaci√≥n de modelos de detecci√≥n de objetos. Con algunos ajustes, creemos que podemos completar con √©xito el fine-tuning en futuras iteraciones.


## 6. Entrenamiento del modelo y comprobaci√≥n del rendimiento. Se entrenar√°n uno o varios modelos, comprobando en cada caso el rendimiento que ofrecen mediante las apropiadas medidas de error y/o acierto.

### 6.1 Uso de YOLOv5 de Ultralytics y Chatbot personalizado

En este apartado, se describe el proceso de implementaci√≥n de YOLOv5 de Ultralytics, desde la configuraci√≥n del entorno hasta la integraci√≥n con una API en Flask y un frontend en React. El objetivo es demostrar c√≥mo este modelo puede ser utilizado para detectar objetos en tiempo real, enviando los resultados de las detecciones a una interfaz gr√°fica que permite visualizar las predicciones de manera intuitiva. 
Adem√°s, se aborda la importancia de optimizar el flujo de trabajo para garantizar un rendimiento √≥ptimo, especialmente como tratar el funcionamiento con recursos gratuitos y el limite que establece Netlify y nuestra API con Flask en local.

Tambi√©n ideamos un chatbot con una API-KEY de OpenAI, donde con base en nuestro README.md, procese y responda preguntas y cuestiones sobre nuestro proyecto debido a, que nuestro readme va a ser bastante extenso.

Primero explicaremos el funcionamiento de YOLOv5 con Ultralytics.

#### Uso de la API con Flask

La API desarrollada con Flask sirve como el n√∫cleo del proyecto, facilitando la comunicaci√≥n entre el modelo de detecci√≥n de objetos YOLOv5 y el chatbot basado en OpenAI. Esta API maneja tanto el procesamiento de im√°genes en tiempo real mediante WebSockets como la interacci√≥n con el chatbot mediante solicitudes REST.

#### Configuraci√≥n del Entorno

Antes de ejecutar la API, es necesario asegurarse de que se tienen instaladas todas las dependencias necesarias. Se pueden instalar mediante:

```bash
pip install -r requirements.txt
python app.py
```

(Aconsejable hacer un environment si necesitas versiones espec√≠fica)

Como la versi√≥n de Python que usamos era la 3.10, con ejecutar el comando de arriba, te instalar√° siempre lo √∫ltimo de esta versi√≥n en concreto de python.

#### Uso de WebSocket para Detecci√≥n de Objetos con YOLOv5

La API emplea flask_socketio para recibir im√°genes desde el frontend en tiempo real, procesarlas con YOLOv5 y devolver las detecciones correspondientes.

#### Flujo de Trabajo

- El frontend env√≠a frames codificados en base64 mediante WebSocket.

- La API recibe y decodifica la imagen, luego la redimensiona para mejorar la eficiencia.

- YOLOv5 procesa la imagen y genera predicciones sobre los objetos detectados.

- La API env√≠a las detecciones de vuelta al frontend a trav√©s de WebSocket.

![api_websocket](https://github.com/user-attachments/assets/0be421ff-2753-4eea-8bb0-560b6aa6c703)

Aunque la anterior imagen representa la funci√≥n y puede ser engorrosa, la siguiente captura ser√° la zona importante y vital de entender.
Esta parte es la m√°s importante ya que sin ella, no podr√≠amos representar en el frontend mediante el uso de canvas, pintar los rect√°ngulos de la detecci√≥n de objetos ya que nos da:
- Las posiciones de cada objeto
- Redonde el score del objeto a 2 decimales
- Gracias a la id, accedemos al nombre de la clase, por ejemplo, 0 - Persona
- A√±adimos esto a una lista finalmente
  
![api_socket2](https://github.com/user-attachments/assets/fe737556-a178-4f62-af07-5f0f63d33886)

#### Uso de API REST para el Chatbot Personalizado

Para permitir que los usuarios interact√∫en con el chatbot, la API implementa un endpoint /chat que recibe preguntas del usuario y responde bas√°ndose en el contenido del README.md del proyecto.

#### Flujo de Trabajo

- El usuario env√≠a una solicitud POST a /chat con el mensaje en formato JSON.

- La API obtiene el contenido del README.md desde GitHub.

- Se construye un mensaje para OpenAI combinando la pregunta del usuario y la informaci√≥n del README.md.

- La API env√≠a la solicitud a OpenAI y devuelve la respuesta generada.

La parte del frontend ser√° expuesta en la secci√≥n **8. Desarrollo de la Aplicaci√≥n Web**

![api_rest](https://github.com/user-attachments/assets/1045acd4-a22b-44f2-ae54-376e16ca642e)

Vemos aqu√≠ mas directamente la parte importante, que usar√° el rol de system, con lo cual nos permite generar un prompt anterior al promt del usuario, donde gracias a esta funci√≥n, transformamos el readme...
![readme](https://github.com/user-attachments/assets/fc3a12d0-7bfc-4919-8424-545b98158e73)

Para conseguir as√≠ finalmente que "sesgemos" al modelo para que responda preguntas con base en nuestro Readme.
![api_chat2](https://github.com/user-attachments/assets/b0b03eec-27d0-43eb-b092-d6e35d641c51)

Las dem√°s lineas de c√≥digo son necesarias para permisos y utilidad como:

**CORS**
![cors1](https://github.com/user-attachments/assets/8f0fc1bd-7060-4089-80d7-5c7b572813e0)
![cors2](https://github.com/user-attachments/assets/cd79b6b5-5ae4-402f-9195-57947fed27bf)

**Optimizaci√≥n YOLO**
![yolo1](https://github.com/user-attachments/assets/15184317-f9f6-4be5-ab08-e465cf3b873e)

---

## 7. Se tiene que incluir alguna de las t√©cnicas estudiadas en el tema de Procesamiento de Lenguaje Natural: expresiones regulares, tokenizaci√≥n, generaci√≥n de texto, an√°lisis de sentimientos, etc.

En el proyecto hemos integrado diversas t√©cnicas de **Procesamiento de Lenguaje Natural (PLN)** para mejorar la interacci√≥n con el usuario y optimizar el an√°lisis de texto.

### 7.1. Uso de Expresiones Regulares en el Formateo de Respuestas del Chatbot

En la p√°gina de `Chatbot.tsx` hemos desarrollado una funci√≥n llamada `formatResponse()`, cuya finalidad es mejorar la legibilidad de los mensajes del chatbot al usuario. Para ello, aplicamos **expresiones regulares** que permiten transformar ciertos patrones de texto en formato HTML.

üìå C√≥digo de la funci√≥n:
![image](https://github.com/user-attachments/assets/3077c79d-38dd-4a5d-8cc7-6e688da97c0c)

Desglose del c√≥digo:

1Ô∏è‚É£ `replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>')`:

- Busca cualquier texto encerrado entre `**` y lo reemplaza por `<strong>`, convirti√©ndolo en negrita.
- Ejemplo: `"Este es un **mensaje importante**"` ‚Üí `"Este es un <strong>mensaje importante</strong>"`.

2Ô∏è‚É£ `replace(/\n/g, '<br>')`:

- Reemplaza los saltos de l√≠nea (`\n`) por etiquetas HTML `<br>`, asegurando que el texto respete los espacios entre l√≠neas.
- Ejemplo: `"L√≠nea 1\nL√≠nea 2"` ‚Üí `"L√≠nea 1<br>L√≠nea 2"`.

3Ô∏è‚É£ `replace(/\d+\. /g, '<br>‚Ä¢ ')`:

- Busca listas numeradas (`1. Texto`, `2. Texto`, etc.) y las convierte en listas con vi√±etas (`‚Ä¢ Texto`).
- Ejemplo:
  
```plaintext
1. Manzana
2. Pera
```
Se transformar√° en:
```html
Copiar
Editar
<br>‚Ä¢ Manzana
<br>‚Ä¢ Pera
```

## 8. Desarrollo de la Aplicaci√≥n Web

Nuestra aplicaci√≥n web ha sido desarrollada utilizando **React** e **Ionic** con **TypeScript**, proporcionando una experiencia moderna y responsiva. A continuaci√≥n, describimos los principales componentes de la web junto con capturas del c√≥digo y la interfaz.

### 8.1 Estructura del Proyecto

El proyecto se organiza en distintos componentes de React y p√°ginas espec√≠ficas para cada funcionalidad. Nuestra estructura principal incluye:
- **Home.tsx** (P√°gina de inicio)
- **Model.tsx** (M√≥dulo de detecci√≥n de objetos)
- **AboutUs.tsx** (Informaci√≥n del equipo)
- **Chatbot.tsx** (Asistente virtual basado en IA)

Con esta organizaci√≥n permitimos una estructura modular y escalable. üöÄ

### 8.2 Inicio (Home.tsx)

Nuestra p√°gina principal (**Home.tsx**) presenta el proyecto y enlaza a su repositorio en GitHub. Hemos utilizado iconos para mejorar la accesibilidad visual.

#### ¬øQu√© hace este archivo?
- Muestra el dise√±o principal: Incluimos el encabezado, el contenido y cualquier elemento visual que queramos resaltar.
- Carga datos si es necesario: Dependiendo de lo que queremos mostrar, aqu√≠ podemos traer informaci√≥n desde una API o una base de datos.
- Facilita la navegaci√≥n: Agregamos enlaces o botones para que los usuarios puedan moverse dentro de nuestra aplicaci√≥n.
  
---

#### 8.2.1 Importaci√≥n de librer√≠as y estilos
El archivo `Home.tsx` es un componente de React que utiliza Ionic y otros elementos para la estructura y dise√±o de la pantalla principal de la aplicaci√≥n.

üìå C√≥digo de importaci√≥n:
![image](https://github.com/user-attachments/assets/b3ce7a56-9c89-4614-93ee-706c37b11121)

---

#### 8.2.2 Contenido de la p√°gina
En la secci√≥n principal de la pantalla, mostramos el t√≠tulo del proyecto junto con una breve descripci√≥n para que los usuarios comprendan su prop√≥sito de inmediato. Adem√°s, proporcionamos enlaces directos a los repositorios de GitHub, tanto para la web como para la API, con iconos interactivos que facilitan el acceso.

üìå C√≥digo del contenido:
![image](https://github.com/user-attachments/assets/1d891c5d-278b-42b3-8b11-5adc13391e66)

---

#### 8.2.3 Estilos Aplicados
En `Home.css`, definimos estilos para mejorar la apariencia del componente. 

üìå Ejemplo de dise√±o:

![image](https://github.com/user-attachments/assets/2b9051a9-4ea8-4f39-bdbf-de611e44f181)

Con estos estilos nos aseguramos que la p√°gina tenga un dise√±o centrado y est√©tico.


‚ú® **Vista de la p√°gina de inicio:**  

![image](https://github.com/user-attachments/assets/a850f36f-605a-406a-a75b-50b4e671ce34)


Esta p√°gina brinda una bienvenida clara y acceso directo a la informaci√≥n clave del proyecto. üöÄ

## 8.3 Modelo de Detecci√≥n de Objetos (Model.tsx)

En esta p√°gina implementamos la detecci√≥n de objetos en tiempo real utilizando la c√°mara del dispositivo. Para ello, hacemos uso de WebSockets para enviar frames al backend y recibir las detecciones procesadas.

#### ¬øQu√© hace este archivo?
- Captura video en tiempo real desde la c√°mara del dispositivo. El c√≥digo a continuaci√≥n solicita permisos para acceder a la c√°mara del dispositivo y captura el video en tiempo real:
  
  ![image](https://github.com/user-attachments/assets/9d704801-afd2-4f15-8cb7-8563a47fdeb5)

- Env√≠a frames al backend. Usa WebSockets para enviar im√°genes a la API, donde se realiza la detecci√≥n de objetos.

  ![image](https://github.com/user-attachments/assets/3045fff8-ff73-4c49-b901-24192edad290)

  
- Recibe y dibuja detecciones. Recibe los resultados del backend y los dibuja sobre el video en un canvas.

  ![image](https://github.com/user-attachments/assets/aa6c6931-02fe-444d-83f4-0db89417b673)

---

#### 8.3.1 Importaci√≥n de librer√≠as y estilos
El archivo Model.tsx importa las siguientes librer√≠as:

- react, useEffect, useRef: Para gestionar el ciclo de vida del componente y referencias.
- socket.io-client: Para la comunicaci√≥n en tiempo real con el backend.
- @ionic/react: Para la estructura de la p√°gina en Ionic.
- @capacitor/core y @capacitor/status-bar: Para ajustar la interfaz en dispositivos m√≥viles.

üìå C√≥digo de importaci√≥n:
![image](https://github.com/user-attachments/assets/44c13030-c11c-450b-8eff-bf19317438da)

---

#### 8.3.2 Contenido de la p√°gina
Esta secci√≥n estructura la interfaz del m√≥dulo:

- Video en vivo: Capturamos la imagen de la c√°mara.
- Canvas de detecciones: Dibujamos los resultados del modelo de IA sobre el video.

üìå C√≥digo del contenido:
![image](https://github.com/user-attachments/assets/a9428b16-d8ba-474a-9211-cdd2a1f4d686)

---

#### 8.3.3 Comunicaci√≥n con el Backend

Conexi√≥n al Backend (A nivel local)
![image](https://github.com/user-attachments/assets/9f55390b-f505-409d-a544-9972425c4e85)

Aqu√≠ se establece la conexi√≥n con el backend en el puerto 5000.

Cuando el backend detecta objetos en el frame enviado, devuelve las coordenadas y la confianza del modelo. Este c√≥digo se encarga de dibujar los resultados sobre el video:

![image](https://github.com/user-attachments/assets/61bd5e5d-8bb5-4451-8c40-a219badb81d9)

Desconecta el socket cuando el usuario sale de la p√°gina:

![image](https://github.com/user-attachments/assets/c6493fce-0297-4b97-a311-39f34df48f0a)

---

#### 8.3.4 Estilos Aplicados
En Model.css, definimos estilos para:

Centrar el video en pantalla.
Ajustar el tama√±o del video y el canvas.
Aplicar un fondo con degradado.

üìå Ejemplo de dise√±o:
![image](https://github.com/user-attachments/assets/c44fcc65-8273-48ec-b1da-ffd645ccf3e7)


‚ú® **Vista del modelo de detecci√≥n de objetos:**

![image](https://github.com/user-attachments/assets/0ca68b6f-3eab-4103-adc5-16d4345aa22f)


Con esta implementaci√≥n logramos un procesamiento √°gil y preciso, permitiendo a los usuarios identificar objetos en tiempo real de manera intuitiva y eficaz. üéØ

## 8.4 Informaci√≥n del Equipo (AboutUs.tsx)

En esta p√°gina mostramos a los integrantes del equipo de desarrollo. En la p√°gina se puede visualizar una lista de miembros, su rol, su formaci√≥n y enlaces a sus perfiles de GitHub y LinkedIn.

‚ú® **Vista de la p√°gina de Informaci√≥n del Equipo:**

![image](https://github.com/user-attachments/assets/e2236a86-5f5f-44dc-82c4-b8aa7872afaa)


## 8.5 Chatbot (Chatbot.tsx)

En esta p√°gina implementamos un chatbot interactivo que permite a los usuarios realizar preguntas. Utilizamos un backend en Node.js para procesar las consultas y devolver respuestas din√°micas.

### ¬øQu√© hace este archivo?
- **Muestra un chatbot en la aplicaci√≥n.**
- **Permite la interacci√≥n con el usuario.** El usuario puede escribir preguntas y recibir respuestas en tiempo real.
- **Env√≠a consultas a un backend en Node.js.** Se conecta a un servidor en `http://localhost:5000/chat` para procesar los mensajes.
- **Formatea las respuestas.** Convierte ciertos elementos de texto como negritas y listas en formato HTML para mejorar la legibilidad.

---

### 8.5.1 Importaci√≥n de librer√≠as y estilos
El archivo `Chatbot.tsx` importa las siguientes librer√≠as:

- **react, useState**: Para gestionar el estado del chatbot y los mensajes.
- **axios**: Para enviar solicitudes HTTP al backend.
- **@ionic/react**: Para la estructura de la p√°gina.
- **Footer**: Componente reutilizable para el pie de p√°gina.
- **Chatbot.css**: Archivo de estilos para la apariencia del chatbot.

üìå **C√≥digo de importaci√≥n:**
![image](https://github.com/user-attachments/assets/8822090b-325f-4af6-aa1f-72307de3ce1f)


### 8.5.2 Estado y manejo de mensajes
Almacenamos los mensajes en un array gestionado con `useState`. Inicialmente, contiene un mensaje de bienvenida del bot:

üìå **C√≥digo de inicializaci√≥n:**
![image](https://github.com/user-attachments/assets/bf0786c2-d521-4084-a8a0-e469a4a0aa52)
![image](https://github.com/user-attachments/assets/59699bd8-f7bf-437c-bffb-d92c8e92b76d)


El usuario puede escribir un mensaje y enviarlo con `sendMessage()`, que realiza las siguientes acciones:
1. Agrega el mensaje del usuario al estado.
2. Env√≠a la consulta al backend mediante una petici√≥n `POST`.
3. Recibe la respuesta del servidor y la formatea.
4. Agrega la respuesta del chatbot a la conversaci√≥n.
5. Limpia el input despu√©s de enviar el mensaje.

üìå **C√≥digo de env√≠o de mensajes:**
![image](https://github.com/user-attachments/assets/f6eabbe7-1417-42dd-88b0-25fa0b26bf11)


---

### 8.5.3 Renderizado del Chatbot

El chatbot se compone de:
- Un **contenedor de mensajes**, donde se muestran las interacciones previas.
- Un **input de texto** para que el usuario escriba su mensaje.
- Un **bot√≥n de env√≠o** para enviar mensajes manualmente.
- La posibilidad de presionar **Enter** para enviar el mensaje.

üìå **C√≥digo del renderizado:**
![image](https://github.com/user-attachments/assets/ef0402ba-5a17-42e8-8ef4-591904aab9c8)


---

### 8.5.4 Formateo de respuestas

Para mejorar la presentaci√≥n de las respuestas, convertimos ciertos elementos a formato HTML:
- **Negritas**: `**texto**` ‚Üí `<strong>texto</strong>`
- **Saltos de l√≠nea**: `\n` ‚Üí `<br>`
- **Listas numeradas**: `1. Texto` ‚Üí `‚Ä¢ Texto`

üìå **C√≥digo de formateo:**
![image](https://github.com/user-attachments/assets/ac19de25-1edd-4c50-b5e9-ac02fcf076a0)



---

‚ú® **Vista del chatbot funcionando:**

![chatbot](https://github.com/user-attachments/assets/915472c2-ab6a-485f-ac20-7fb3b2ff22d0)

Con esta implementaci√≥n ofrecemos una experiencia fluida y responsiva, permitiendo a los usuarios interactuar con el asistente virtual de manera sencilla y eficiente. üöÄ

